{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvy4JiSgX-wB"
   },
   "source": [
    "# <center> Unsupervised Learning Methods </center>\n",
    "## <center> Problem Set 1 </center>\n",
    "### <center> Optimization and Clustering </center>\n",
    "\n",
    "\n",
    "Ady Michelson Id:308563345\n",
    "\n",
    "Yossi Gavriel\n",
    "305498099\n",
    "\n",
    "\n",
    "\n",
    "#1.1.1\n",
    "\n",
    "Prove:\n",
    "let $x∈R^{d}≥0$ and $y∈R^{d}≥0$,\n",
    "we want to prove that for each $a∈[0,1]$ $ax +(1-a)y∈ R^d≥0$.\n",
    "\n",
    "If $a=0 => ax +(1-a)y = y ∈ R^d≥0$\n",
    "\n",
    "If $a=1 => ax +(1-a)y = x ∈ R^d≥0$\n",
    "\n",
    "If $a > 0 $ and  $a<1 $ \n",
    " $ =>$    \n",
    " $ax ≥ 0$ because of min $x_i≥ $ 0   and $(1-a)y ≥ 0 $ because of min $ y_i≥ 0$    $=> ax +(1-a)y ≥ 0 $.\n",
    "\n",
    "Let $ax +(1-a)y = m$ , min $m_i ≥ 0$  $=> m∈ R^d ≥ 0$\n",
    "And that why $R^d ≥ 0$ is convex.\n",
    "#1.1.2\n",
    "We will prove with induction.\n",
    "\n",
    "Basic : let $n=2$, $x1∈C$ and $x2∈C$  \n",
    "$=>$ $x1a1 +x2a2 ∈C$ because $x1a + (1-a)x2 ∈C$ \n",
    "\n",
    "base of the definition of convex, $a1,a2 ∈[0,1]$\n",
    " $=>$  $a1,a2 ≥0$  and $a1+a2 =1$.\n",
    "\n",
    "Induction assumption: assume a combination of  $ \\Sigma^{n}_{i=1} x_ia_i$ ∈C\n",
    "\n",
    "Prove for N:\n",
    "\n",
    "We will prove that  $ \\Sigma^{n}_{i=1} x_ia_i$ ∈C and  $ \\Sigma^{n+1}_{i=1} a_i =1$\n",
    "\n",
    "and for each $a_i i≥1$ and $i≤N+1 $ $a_i ≥0$.\n",
    "\n",
    "\n",
    "$ \\Sigma^{n}_{i=1} x_ia_i  =\n",
    "x_1a_1 +x_2a_2 + …+x_na_n = (a_1+a_2+…+a_n-1)((\\frac{a_1}{ a_1+a_2+…+a_n-1})x_1 + (\\frac{a_2}{ a_1+a_2+…+a_n-1})x_2+…+(\\frac{a_n-1}{ a_1+a_2+…+a_n-1})x_{n-1}) + a_nx_n$\n",
    "\n",
    "\\\n",
    "Let M = $(\\frac{a_1}{a_1+a_2+…+a_n-1})$$x_1$ + $(\\frac{a_2}{ a_1+a_2+…+a_n-1})$$x_2$+…+$(\\frac{a_n-1}{ a_1+a_2+…+a_n-1})$$x_{n-1}$\n",
    "\n",
    "Base of the Induction assumption a combination of $N-1$ $(\\frac{a_1}{ a_1+a_2+…+a_n-1})$$x_1$ + $(\\frac{a_2}{ a_1+a_2+…+a_n-1})$$x_2$+…+$(\\frac{a_n-1}{ a_1+a_2+…+a_n-1})$$x_{n-1}$ $∈C$ $=>$  $M∈C$, $a1_+a_2+…+a_n =1$\n",
    "\n",
    "$a_1+a_2+…+a_n-1= 1-a_n$ $=>$ base of the definition of convex that $(a_1+a_2+…+a_n-1)$$M$ + $a_n*x_n ∈C$ because $M∈C$ and $x_n∈C$.\n",
    "\n",
    "#1.1.3:\n",
    "Let $C = R^2/{(0,0)}$ contained in $R^2$ , $y = [10,4] ∈C$, ${[-1,-4], [-2,-9], [-4,-1],[-9,-6],[-3,-3],[-5,-5],[-8,-8],[-2,-2],[-9,-7],[-7,-7]}$ , we will prove that there are no convex combination such $[10,4] = a_1[-1,-4] + a_2[-2,-9]+a_3[-4,-1] + a_4[-9,-6]+ a_5[-3,-3]+a_6[-5,-5]+a_7[-8,-8]+a_8[-2,-2]+a_9[-9,-7]+a_10[-7,-7]$ because each $a_i ≥0$ and all the vectors are negative and the sum of negative is also vegetive and we will not get the vector $[10,4]$\n",
    "\n",
    "\n",
    "# 1.1.4\n",
    "First, multiply 1 by $\\alpha$ and 2 by $1 - \\alpha$: \n",
    "\n",
    "$$\\alpha f(y) \\geq \\alpha (f(z)+f^{\\prime}(z)(y-z))$$\n",
    "\n",
    "$$(1-\\alpha) f(x) \\geq (1-\\alpha) (f(z)+f^{\\prime}(z)(x-z))$$\n",
    "\n",
    "Now, adding 1 & 2:\n",
    "\n",
    "$$\n",
    "\\alpha f(y) + (1-\\alpha) f(x) \\geq \\alpha (f(z)+f^{\\prime}(z)(y-z)) + (1-\\alpha) (f(z)+f^{\\prime}(z)(x-z))\n",
    "$$\n",
    "$$\n",
    "\\alpha f(y) + (1-\\alpha) f(x) \\geq \\alpha (f(z)+f^{\\prime}(z)(y-z)) + (1-\\alpha) (f(z)+f^{\\prime}(z)(x-z))\n",
    "$$\n",
    "$$\n",
    "\\alpha f(y) +  f(x) - \\alpha f(x) \\geq  f(z) + \\alpha f(z)+ \\alpha f^{\\prime}(z)(y-z) +f^{\\prime}(z)(x-z) -\\alpha f(z) -\\alpha f^{\\prime}(z)(x-z) =>\n",
    "$$\n",
    "$$ \\alpha f(y) +  f(x) - \\alpha f(x) \\geq f(z) + f'(z) \\left(\\alpha(y-z) + (x-z) -\\alpha(x-z) \\right) =>$$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f(z) + f'(z) \\left(\\alpha y-z\\alpha  + x-z -\\alpha x+\\alpha z \\right) =>$$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f(z) + f'(z) \\left(\\alpha y  + x-z -\\alpha x \\right) =>$$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f(z) + f'(z) \\left((1-\\alpha)x + \\alpha y -z  \\right) =>$$\n",
    "Now, sience $z = (1-\\alpha)x + \\alpha y$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f(z) + f'(z) \\left(z -z  \\right) => $$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f(z) => $$\n",
    "$$ \\alpha f(y) + (1-\\alpha) f(x) \\geq f((1-\\alpha)x + \\alpha y) => $$\n",
    "$$ f(\\alpha y + (1-\\alpha)x ) \\leq \\alpha f(y) + (1-\\alpha) f(x)   => $$\n",
    "\n",
    " f is a convex function based on defenition\n",
    "# 1.2.1\n",
    "\n",
    "$\\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f\\left(\\boldsymbol{x}_{0}\\right)[\\boldsymbol{h}]=\\left\\langle\\boldsymbol{g}_{0}, \\boldsymbol{h}\\right\\rangle \\Longrightarrow \\boldsymbol{g}_{0}=\\nabla f\\left(\\boldsymbol{x}_{0}\\right)$\n",
    "\n",
    "$\\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla \\delta\\left(\\boldsymbol{x}_{0}\\right)[\\delta_{1}]=\\left\\langle\\boldsymbol{g}_{0}, \\boldsymbol{\\delta_{1}}\\right\\rangle \\Longrightarrow \\boldsymbol{g}_{0}[1]=\\nabla f\\left(\\boldsymbol{x}_{0}\\right)$\n",
    "\n",
    "where $ \\boldsymbol{g}_{0} =  \\begin{bmatrix}\\boldsymbol{g}_{0}[0] \\\\ \\boldsymbol{g}_{0}[1] \\\\ .. \\\\ \\boldsymbol{g}_{0}[n]\\end{bmatrix}$\n",
    "\n",
    "and \n",
    "\n",
    "$ \\boldsymbol{\\delta}_{1} =  \\begin{bmatrix}1 \\\\ 0 \\\\ .. \\\\ 0\\end{bmatrix}$\n",
    "\n",
    "It's true for every h so\n",
    "\n",
    "$\\begin{aligned} \\nabla f(x_{1})[\\delta_{1}] &= \\lim _{t \\rightarrow 0} \\frac{f(x_{0}+t \\delta_{1})-f(x_{0})}{t}\\end{aligned} = \\frac{\\mathrm{d} f}{\\mathrm{d} x_{1}} (x_{0}) $\n",
    "\n",
    "\n",
    "\n",
    "$\\begin{aligned} \\frac{\\partial f}{\\partial x_{3}}(\\underline{x})=\\nabla f(\\underline{x})\\left[\\delta_{3}\\right] = &\\left\\langle g_{0} , \\delta_{3}\\right\\rangle=g_{0}[3] \\\\ &=>g_{0}=\\left[\\begin{array}{l}\\frac{\\partial f}{\\partial x_{1}} (\\underline{x}) \\\\ \\frac{\\partial f}{\\partial x_{2}} (\\underline{x}) \\\\ .. \\\\ \\frac{\\partial f}{\\partial x_{n}} (\\underline{x})\\end{array}\\right]\\end{aligned} = \\nabla f\\left(\\boldsymbol{x}_{0}\\right)$\n",
    "\n",
    "# 1.2.2\n",
    "\n",
    "$\\nabla f(x)[h]=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(x+t h)-f(x)}{t}$\n",
    "\n",
    "from linearity of f:\n",
    "\n",
    "$\\nabla f(x)[h]=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(x)+t f(h)-f(x)}{t} = \\frac{\\operatorname{t} f(h)}{t}=f(h)$\n",
    "# 1.2.3\n",
    "\n",
    "# 1. \n",
    "\n",
    "$f(x)=\\langle\\underline{x}, \\underline{A} x\\rangle$\n",
    "\n",
    "\n",
    "$\\nabla f(\\underline{x})[h]=\\left\\langle\\underline{h},\\underline{A} \\underline{x}\\right\\rangle+\\left\\langle\\underline{x}, \\underline{A} h\\right\\rangle =\\left\\langle\\underline{h},\\underline{A} \\underline{x}\\right\\rangle+\\left\\langle \\underline{A^{T}x}, h\\right\\rangle = $\n",
    "\n",
    "$ \\left\\langle {(A + A^{T}) x}, h\\right\\rangle $ \n",
    "\n",
    "$\\nabla f\\left(x\\right) = (A + A^{T}) x$\n",
    "# 2.\n",
    "\n",
    "$f({X})=\\operatorname{Tr}\\left\\{{X}^{T} {A} {X}\\right\\} =\\langle\\underline{X}, \\underline{A} X\\rangle$\n",
    "\n",
    "\n",
    "$\\nabla f(\\underline{X})[h]=\\left\\langle\\underline{h},\\underline{A} \\underline{X}\\right\\rangle+\\left\\langle\\underline{X}, \\underline{A} h\\right\\rangle =\\left\\langle\\underline{h},\\underline{A} \\underline{X}\\right\\rangle+\\left\\langle \\underline{A^{T}X}, h\\right\\rangle = $\n",
    "\n",
    "$ \\left\\langle {(A + A^{T}) X}, h\\right\\rangle $ \n",
    "\n",
    "$\\nabla f\\left(X\\right) = (A + A^{T}) X$\n",
    "# 3. \n",
    "\n",
    "In 1D is:\n",
    "\n",
    "$f(x)=||y-A x||^{2} = \\langle y-Ax, {y-Ax}\\rangle$\n",
    "\n",
    "$\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] = \\langle -Ah, {y-Ax}\\rangle +\\langle y-Ax, {-Ah}\\rangle= \\langle-2 A^{T}(y-A x), {h}\\rangle$\n",
    "\n",
    "$\\nabla f(x)=-2 A^{T}(y-A x)$\n",
    "\n",
    "\n",
    "# 4. \n",
    "\n",
    "In 1D is:\n",
    "\n",
    "$f(X)=||Y-A X||^{2} = \\langle Y-AX, {Y-AX}\\rangle$\n",
    "\n",
    "$\\nabla f(\\boldsymbol{X})[\\boldsymbol{H}] = \\langle -AH, {Y-AX}\\rangle +\\langle Y-AX, {-AH}\\rangle= \\langle-2 A^{T}(Y-A X), {H}\\rangle$\n",
    "\n",
    "$\\nabla f(X)=-2 A^{T}(Y-A X)$\n",
    "\n",
    "#5.\n",
    "\n",
    "$f(X)=\\langle X^{T}A, Y^{T}\\rangle = A^{T}XY^{T}$\n",
    "\n",
    "$\\nabla f(X)[H]=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(X+tH)-f(x)}{t} = \\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{} A^{T}(X+tH)Y^{T}- A^{T}XY^{T}}{t}=\\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}A^{T}XY^{T} + A^{T}tHY^{T}- A^{T}XY^{T}}{t}= \\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}A^{T}tHY^{T}}{t} = A^{T}HY^{T} = \\langle A, HY^{T}\\rangle = \\langle AY, H\\rangle   $ \n",
    "\n",
    "$\\nabla f(X)=AY$\n",
    "\n",
    "#6.\n",
    "\n",
    "\n",
    "$f(x)=a^{T}g(x)$\n",
    "\n",
    "\n",
    "$\\nabla f(\\boldsymbol{x})[\\boldsymbol{h}] =\\operatorname{lim}_{t \\rightarrow 0}\\frac{f(x+th)-f(x)}{t}=\\operatorname{lim}_{t \\rightarrow 0}\\frac{a^{T}g(x+th)-a^{T}g(x)}{t} = \\operatorname{lim}_{t \\rightarrow 0}\\frac{a^{T}(g(x+th)-g(x))}{t} =  \\operatorname{lim}_{t \\rightarrow 0}\\frac{a^{T}(\\ \\begin{bmatrix}\\boldsymbol{g}(x1+th) \\\\ \\boldsymbol{g}(x2+th) \\\\ .. \\\\ \\boldsymbol{g}(xn +th)\\end{bmatrix}-\\ \\begin{bmatrix}\\boldsymbol{g}(x1) \\\\ \\boldsymbol{g}(x2) \\\\ .. \\\\ \\boldsymbol{g}(xn)\\end{bmatrix})}{t} =  \\operatorname{lim}_{t \\rightarrow 0}\\frac{a^{T}(\\ \\begin{bmatrix}\\boldsymbol{g}(x1+th)-g(x_1) \\\\ \\boldsymbol{g}(x2+th)-g(x_2) \\\\ .. \\\\ \\boldsymbol{g}(xn +th)- g(x_n)\\end{bmatrix})}{t} = a^{T}\\ \\begin{bmatrix}\\boldsymbol{g'}(x1)[h_1] \\\\ \\boldsymbol{g'}(x2)[h_2] \\\\ .. \\\\ \\boldsymbol{g'}(xn)[h_n]\\end{bmatrix} = a^{T}g'(x)[h] $\n",
    "\n",
    "\n",
    "$\\nabla f(x)=a^{T}g'(x)$\n",
    "\n",
    "#7.\n",
    "\n",
    "$f(X)=\\langle A, log[X]\\rangle = Tr{Alog[X]}$\n",
    "\n",
    "$\\nabla f(X)[H]=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(X+tH)-f(x)}{t} = \\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}  Tr{Alog[X+tH]}-  Tr{Alog[X]}}{t}=\\operatorname{lim}_{t \\rightarrow 0}Tr\\frac{\\operatorname{}A(log[X+tH] -log[X])}{t}= Tr{A\\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}(log[X+tH] -log[X])}{t}} = Tr{A\\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}log\\frac{\\operatorname{}X+tH}{X}}{t}}= Tr{A\\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}log\\frac{\\operatorname{}X+tH}{X}}{t}} = Tr{A\\operatorname{lim}_{t \\rightarrow 0}(\\frac{\\operatorname{}1}{t}log\\frac{\\operatorname{}X+tH}{X})}=  Tr{A\\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}log\\frac{\\operatorname{}X+tH}{X}}{t}} = Tr{A\\operatorname{lim}_{t \\rightarrow 0}(log\\frac{\\operatorname{}X+tH}{X})}^\\frac{1}{t}=TrAlog[e^\\frac{H}{X}] = TrA\\frac{H}{X}=TrA\\frac{1}{X}H =\\langle A[\\frac{1}{X}], H\\rangle $\n",
    "\n",
    "$\\nabla f(X)=A[\\frac{1}{X}]$\n",
    "\n",
    "#8.\n",
    "\n",
    "$f(X)=\\langle a, diag(X)\\rangle = Tr {a diag(X)}$\n",
    "\n",
    "$\\nabla f(X)[H]=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(X+tH)-f(x)}{t} = \\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}  Tr {a diag(X+tH)}-   Tr {a diag(X)}}{t}=\\operatorname{lim}_{t \\rightarrow 0} \\frac{f(X+tH)-f(x)}{t} = \\operatorname{lim}_{t \\rightarrow 0}\\frac{\\operatorname{}  aTr {diag(X+tH)}-  a Tr {diag(X)}}{t}= \\operatorname{lim}_{t \\rightarrow 0}Tr\\frac{\\operatorname{}  a{diag(X+tH)}-  a{diag(X)}}{t}=\\operatorname{lim}_{t \\rightarrow 0}Tr a\\frac{\\operatorname{}  {diag(X)+diag(tH)}-  {diag(X)}}{t} = Tr{{ a diag(H)}}= \\langle a, diag(H)\\rangle =\\langle diag(a), H\\rangle=$\n",
    "\n",
    "$\\nabla f(X)=diag(a)$\n",
    "# 1.4.1\n",
    "\n",
    "#1.\n",
    "$minmaxG(x,y) =1$\n",
    "$G = sin(x+y)$\n",
    "\n",
    "The max value of any sin(x) for any x is 1 $=> $ maxG(x,y) = 1 $=>$ min 1 = 1 \n",
    "=> $minmaxG(x,y) =1$\n",
    "\n",
    "#2.\n",
    "The min value of any sin(x) for any x is -1 $=> $ minG(x,y) = 1 $=>$ max -1 = -1 \n",
    "=> $maxminG(x,y) = -1$\n",
    "\n",
    "\n",
    "# 1.4.2\n",
    "\n",
    "# 1.\n",
    "\n",
    "$x^{T}x = \\langle x, x\\rangle  $\n",
    "\n",
    "$||x||^2= \\langle x, x\\rangle  $\n",
    "\n",
    "$||x||^2 = 1 => f(x) = x^{T}Ax$ $=> min f(x) = min  x^{T}Ax$\n",
    "\n",
    "\n",
    "# 2. \n",
    "\n",
    "$\\mathcal{L}(\\lambda):=\\min _{\\boldsymbol{x}} f(\\boldsymbol{x})+\\lambda g(\\boldsymbol{x})$\n",
    "\n",
    "$ f(x) = \\min _{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x}$\n",
    "\n",
    "$ g(x) = \\|\\boldsymbol{x}\\|_{2}^{2} - 1$\n",
    "\n",
    "$\\mathcal{L}(\\lambda):= \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} -\\lambda (\\|\\boldsymbol{x}\\|_{2}^{2} - 1) $\n",
    "# 3. \n",
    "\n",
    "$\\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda)=0 \\Longleftrightarrow \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$\n",
    "\n",
    "1. \n",
    "\n",
    "$$\\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda)=0 \\rightarrow \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$$\n",
    "\n",
    "\n",
    "$$ \\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda) = (A + A^{T}) x -  \\lambda 2 x  = 2 A X - 2\\lambda x \\rightarrow $$\n",
    "\n",
    "$$ 2 A X = 2\\lambda x \\rightarrow$$\n",
    "\n",
    "$$ A X = \\lambda x$$\n",
    "\n",
    "\n",
    "2. \n",
    "\n",
    "$$\\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x} \\rightarrow \\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda)=0$$\n",
    "\n",
    "$$ \\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda) = 2 A X - 2\\lambda x \\rightarrow $$\n",
    "\n",
    "Plug in the fact that $ \\boldsymbol{A} \\boldsymbol{x}=\\lambda \\boldsymbol{x}$ \n",
    "\n",
    "and get: \n",
    "\n",
    "$$ \\nabla_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\lambda) = 2 \\lambda x - 2\\lambda x  = 0 $$\n",
    "# 2.1.1\n",
    "\n",
    "we will prove that :\n",
    "\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2}= \n",
    "\\arg \\min _{\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K}min\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2}$\n",
    "\n",
    "In K means for each $x_i$ we we find the min distance from each centeroid, we assign $x_i$ to the closest centroid .in (2) we find the min of each $x_i$ from\n",
    "from all the centroid and assign that $x_i$ to the cluser of the closet centroid. we can see that both expressions claculte the closest centroid for eanch $x_i$ \n",
    "\n",
    "\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} = \\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} {x_j}\\ \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x_j}\\right\\|_{2}^{2}$\n",
    "\n",
    "In the rigth expression for each cluster we find the distance between all pairs  in the cluser and we minimize the mean of those distance of each cluster , when we do that we will find for each point the closeset centeriod and the what the K means does  \n",
    "# 2.1.2\n",
    "\n",
    "As showed in the lecture when we learned GMM in that example it doesn't converges to a global minimum:\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "#2.2.1.\n",
    "\n",
    "$f_{\\underline{X}}(\\boldsymbol{x})=\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\Sigma|}} \\exp \\left(-\\frac{1}{2}(\\boldsymbol{x}-\\mu)^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\mu)\\right)$\n",
    "\n",
    "$f_{\\underline{Y}}(\\boldsymbol{y})=\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\Sigma|}} \\exp \\left(-\\frac{1}{2}(\\boldsymbol{y}-a^{T}E[X]+b)^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}-a^{T}E[X]+b)\\right)$\n",
    "\n",
    "$f_{\\underline{Y}}(\\boldsymbol{y})=\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\Sigma|}} \\exp \\left(-\\frac{1}{2}(\\boldsymbol{y}-a^{T}\\mu_x+b)^{T} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}-a^{T}\\mu_x+b)\\right)$\n",
    "# 2.2.2\n",
    "\n",
    "\n",
    "\n",
    "For a sample of vectors $x_{i}=\\left(x_{i 1}, \\ldots, x_{i k}\\right)^{\\top}$, with $i=1, \\ldots, n$, the sample mean vector is\n",
    "$$\n",
    "\\bar{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\n",
    "$$\n",
    "and the sample covariance matrix is\n",
    "$$\n",
    "A = \\boldsymbol{\\Sigma}=\\mathbb{E}\\left[(\\underline{X}-\\boldsymbol{\\mu})(\\underline{X}-\\boldsymbol{\\mu})^{T}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{\\mu}\\right)\\left(x_{i}-\\bar{\\mu}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "Symetric - \n",
    "\n",
    "\n",
    "$$ \\boldsymbol{\\Sigma}^{T} = $$\n",
    "\n",
    "$$ \\left(\\frac{1}{n} \\sum\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\top}\\right)^{T} = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum \\left(\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\top}\\right)^{T}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum \\left(\\left(x_{i}-\\mu\\right)^{T}\\right)^{T} \\left(x_{i}-\\mu\\right)^{T}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{\\mu}\\right)\\left(x_{i}-\\bar{\\mu}\\right)^{\\top} = \\Sigma$$\n",
    "Non zero second way: \n",
    "\n",
    "$$ v^{\\top} A v  = $$\n",
    "\n",
    "$$ E\\left[\\mathbf{v}^{T}(\\mathbf{X}-\\mathbf{\\mu})(\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right] = $$\n",
    "\n",
    "$$ E\\left[\\left((\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right)^{T}\\left((\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right)\\right] = $$\n",
    "\n",
    "(from symmetrically)\n",
    "\n",
    "$$ E\\left\\{\\left[(\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right]^{2}\\right\\}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\left(x_{i}-\\bar{\\mu}\\right)^{\\top} v\\right)^{2} \\geq 0 $$\n",
    "\n",
    "# 2.3.1\n",
    "\n",
    "remember that metric definition is as the following: \n",
    "\n",
    "A metric on a set $X$ is a function (called distance function or simply distance)\n",
    "$$\n",
    "d: X \\times X \\rightarrow[0, \\infty)\n",
    "$$\n",
    "where $[0, \\infty)$ is the set of non-negative real numbers and for all $x, y, z \\in X$, the following three axioms are satisfied:\n",
    "1. $d(x, y)=0 \\Leftrightarrow x=y$\n",
    "identity of indiscernibles\n",
    "2. $d(x, y)=d(y, x) \\quad$ symmetry\n",
    "3. $d(x, y) \\leq d(x, z)+d(z, y) \\quad$ triangle inequality\n",
    "\n",
    "# 1. \n",
    "\n",
    "if\n",
    "$ C_{1} == C_{2} $  => $ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right) = 0 $\n",
    "\n",
    "by definition \n",
    "\n",
    "$ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right) = 0 => C_{1} == C_{2} $\n",
    "\n",
    "\n",
    "# 2.\n",
    "\n",
    "$$ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right)  =  $$\n",
    "$$\\max _{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right\\|_{2}^{2} = $$\n",
    "\n",
    "\n",
    "$$ \\max _{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}}\\left\\|\\boldsymbol{x}_{j}-\\boldsymbol{x}_{i}\\right\\|_{2}^{2} = $$ \n",
    "\n",
    "$$d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{2}, \\mathcal{C}_{1}\\right)$$\n",
    "\n",
    "# 3. \n",
    "\n",
    "We know that norm is satisfied Triangle inequality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbj-_HM_G-Jl"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88fJJXLYX-wm"
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('font', **{'size' : 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLKJcTIoX-wq"
   },
   "source": [
    " # 1. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtkW84ngX-ws"
   },
   "source": [
    "## 1.3 Descent Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5XhscWjX-wt"
   },
   "source": [
    "### Gradient descent\n",
    "* Let $\\boldsymbol{Y}\\in\\mathbb{R}^{d\\times d}$ be a blurred version\n",
    "of the image $\\boldsymbol{X}\\in\\mathbb{R}^{d\\times d}$:\n",
    "$$\\boldsymbol{Y}=\\boldsymbol{H}\\boldsymbol{X}\\boldsymbol{H}^{T}$$\n",
    "where $\\boldsymbol{H}\\in\\mathbb{R}^{d\\times d}$ is a known (separable)\n",
    "blurring matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rbIzihlX-wv"
   },
   "source": [
    "#### Load the data ($\\boldsymbol{Y}$ and $\\boldsymbol{H}$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "f9xtIa0RX-ww",
    "outputId": "64781427-3942-40ee-ec92-9db99e2d28f7"
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "#from google.colab import drive\n",
    "#drive.mount(\"/content/drive/\")\n",
    "\n",
    "\n",
    "data = loadmat('data.mat')\n",
    "Y    = data['Y']\n",
    "H    = data['H']\n",
    "\n",
    "Y.shape, H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hX0DMgtX-w1"
   },
   "source": [
    "#### Plot $Y$ and $H$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "yHpdcC2JX-w2",
    "outputId": "7be2c4df-c385-4c27-a3fd-3e34e23d256e"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(Y, cmap='gray')\n",
    "ax[1].matshow(H)\n",
    "ax[0].set_title('$Y = H X H^T$')\n",
    "ax[1].set_title('$H$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlYLpRm8X-w5"
   },
   "source": [
    "* Our goal is to find $\\boldsymbol{X}$ that minimizes:\n",
    "$$\\boldsymbol{X}^{\\star}=\\arg\\min_{\\boldsymbol{X}}f\\left(\\boldsymbol{X}\\right)=\\arg\\min_{\\boldsymbol{X}}\\left\\Vert \\boldsymbol{Y}-\\boldsymbol{H}\\boldsymbol{X}\\boldsymbol{H}^{T}\\right\\Vert _{F}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7OU0ILiX-w7"
   },
   "source": [
    "### 1.3.1\n",
    "1. Find a closed form expression for $\\boldsymbol{X}^{\\star}$ as a function of $\\boldsymbol{Y}$ and $\\boldsymbol{H}$.\n",
    "2. Compute $\\boldsymbol{X}^{\\star}$ and plot it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrrrpHp2X-w8"
   },
   "source": [
    "$\\left.\\begin{array}{l}f(x)= (y-h x h^{T})^{2} =  \\\\\n",
    "f(x)= (y-h h x^{T})^{2} =  \\\\\n",
    "f(x)= (y-h^{2} x^{T})^{2} =  \\\\\n",
    " f^{\\prime}(x)=-2 h^{2} (y-h x h^{T})\\end{array}\\right\\} 1 \\mathrm{D}$\n",
    "\n",
    "\n",
    "$\\nabla f(x)=-2 H H^{T} (y-H X H^{T})$\\\n",
    "$\\nabla f(x) = 0$\\\n",
    "$ (Y-H X H^{T}) = 0$\\\n",
    "$X=H^{-1}Y(H^{T})^{-1}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "wHtnm0g3X-w9",
    "outputId": "e363d233-68a2-422b-8500-f05fad496d78"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "Xstar = np.linalg.inv(H)@Y@np.linalg.inv(H.T)\n",
    "\n",
    "\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(Y,     cmap='gray')\n",
    "ax[1].imshow(Xstar, cmap='gray')\n",
    "ax[0].set_title('$Y = H X H^T$')\n",
    "ax[1].set_title('$X^{\\star}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9H8fybdX-w_"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "* To avoid matrix inversion, we will find $\\boldsymbol{X}^{\\star}$ using gradient descent.\n",
    "### 1.3.2\n",
    "1. Implement the following functions where\n",
    "$$f\\left(\\boldsymbol{X}\\right)=\\left\\Vert \\boldsymbol{Y}-\\boldsymbol{H}\\boldsymbol{X}\\boldsymbol{H}^{T}\\right\\Vert _{F}^{2}$$\n",
    "\n",
    "```python\n",
    "#==================================================#\n",
    "def f(X):\n",
    "    '''\n",
    "    f(X) = ||Y - HXH^T||_F^2 is the objective function\n",
    "    '''\n",
    "    pass\n",
    "#==================================================#\n",
    "#==================================================#\n",
    "def Df(X):\n",
    "    '''\n",
    "    Df(X) = ∇f(X) is the objective gradient\n",
    "    '''\n",
    "    pass\n",
    "#==================================================#\n",
    "#==================================================#\n",
    "def ApplyGradientDescent(f, Df, X0, lr, ε):\n",
    "    '''\n",
    "    Apply gradient descent.\n",
    "    Args:\n",
    "        f  - Objective function\n",
    "        Df - Objective gradient\n",
    "        X0 - Initial point\n",
    "        lr - Learning rate (step size)\n",
    "        ε  - Stopping criterion value\n",
    "    Output:\n",
    "        X  - The converged X\n",
    "    Stopping criterion:\n",
    "        Stop iterate when f(X) < ε\n",
    "    '''\n",
    "    pass\n",
    "#==================================================#\n",
    "```\n",
    "\n",
    "2. Set a reasonable starting point $\\boldsymbol{X}^{\\left(0\\right)}$.\n",
    "3. Find $\\boldsymbol{X}$ such that $f\\left(\\boldsymbol{X}\\right)<\\epsilon$ where $\\epsilon=10^{-4}$.\n",
    "    * Time your function.\n",
    "    * Plot the final obtained $\\boldsymbol{X}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKqg0p4TX-xC"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "MAX_ITERATIONS = 100000\n",
    "\n",
    "def f(X):\n",
    "    '''\n",
    "    f(X) = ||Y - HXH^T||_F^2 is the objective function\n",
    "    '''\n",
    "    \n",
    "    return np.power(np.abs(Y - H*X*H.T), 2)\n",
    "\n",
    "def Df(X):\n",
    "    '''\n",
    "    Df(X) = ∇f(X) is the objective gradient\n",
    "    '''\n",
    "    return -2* H@H.T@(Y-H@X@H.T)\n",
    "\n",
    "def ApplyGradientDescent(f, Df, X0, lr, ε):\n",
    "    '''\n",
    "    Apply gradient descent.\n",
    "    Args:\n",
    "        f  - Objective function\n",
    "        Df - Objective gradient\n",
    "        X0 - Initial point\n",
    "        lr - Learning rate (step size)\n",
    "        ε  - Stopping criterion value\n",
    "    Output:\n",
    "        X  - The converged X\n",
    "    Stopping criterion:\n",
    "        Stop iterate when f(X) < ε\n",
    "    '''\n",
    "    cur_x = X0\n",
    "    current_error = np.sum(cur_x)\n",
    "    iteration =0\n",
    "    while current_error > ε and iteration < MAX_ITERATIONS:\n",
    "\n",
    "        iteration +=1\n",
    "        prev_x = cur_x.copy()\n",
    "        cur_x +=  - lr * Df(prev_x)\n",
    "        current_error = np.linalg.norm(Y- H@cur_x@H.T)**2\n",
    "\n",
    "    return cur_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNwfB6dBX-xE"
   },
   "outputs": [],
   "source": [
    "X0 = Y.copy()\n",
    "lr = 0.001\n",
    "ε = math.pow(10,-4)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "new_x = ApplyGradientDescent(f, Df, X0, lr, ε)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "GradientDescent_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "M4k7aUC-X-xG",
    "outputId": "6e8ef4ee-b236-40c3-eff5-d53e7fe2a34d"
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(Y,     cmap='gray')\n",
    "ax[1].imshow(new_x, cmap='gray')\n",
    "ax[0].set_title('$Y = H X H^T$')\n",
    "ax[1].set_title('$X^{\\star}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dag38BFwX-xI"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "### Momentum\n",
    "* <ins>Momentum</ins> is an effective method to improve convergence rate.\n",
    "* Gradient descent update rule:\n",
    "$$\\boldsymbol{x}^{\\left(t\\right)}=\\boldsymbol{x}^{\\left(t-1\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)$$\n",
    "* Momentum update rule:\n",
    "$$\\boldsymbol{x}^{\\left(t\\right)}=\\boldsymbol{x}^{\\left(t-1\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-1\\right)}-\\boldsymbol{x}^{\\left(t-2\\right)}\\right)$$\n",
    "where $\\beta\\in[0,1)$ and typically $0.9\\leq\\beta\\leq0.99$.\n",
    "\n",
    "### 1.3.3\n",
    "Show that $\\boldsymbol{x}^{\\left(t\\right)}$ can be expressed by:\n",
    "$$\\boldsymbol{x}^{\\left(t+1\\right)}=\\boldsymbol{x}^{\\left(t\\right)}-\\mu\\sum_{s=0}^{t}\\beta^{t-s}\\nabla f\\left(\\boldsymbol{x}^{\\left(s\\right)}\\right)$$\n",
    "\n",
    "\n",
    "$$\\boldsymbol{x}^{\\left(t + 1\\right)}=\\boldsymbol{x}^{\\left(t\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t\\right)}-\\boldsymbol{x}^{\\left(t-1\\right)}\\right)$$\n",
    "\n",
    "set $ x^{t} = \\boldsymbol{x}^{\\left(t-1\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-1\\right)}-\\boldsymbol{x}^{\\left(t-2\\right)}\\right)$\n",
    "\n",
    "$${x}^{\\left(t + 1\\right)}={x}^{\\left(t\\right)}-\\mu\\nabla f\\left({x}^{\\left(t\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-1\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-1\\right)}-\\boldsymbol{x}^{\\left(t-2\\right)}\\right)-{x}^{\\left(t-1\\right)}\\right)$$\n",
    "\n",
    "$${x}^{\\left(t + 1\\right)}={x}^{\\left(t\\right)}-\\mu\\nabla f\\left({x}^{\\left(t\\right)}\\right)+\\beta\\left(-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-1\\right)}-\\boldsymbol{x}^{\\left(t-2\\right)}\\right)\\right)$$\n",
    "\n",
    "set $ x^{t-1} = \\boldsymbol{x}^{\\left(t-2\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-2\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-2\\right)}-\\boldsymbol{x}^{\\left(t-3\\right)}\\right)$\n",
    "\n",
    "\n",
    "$${x}^{\\left(t + 1\\right)}={x}^{\\left(t\\right)}-\\mu\\nabla f\\left({x}^{\\left(t\\right)}\\right)+\\beta\\left(-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-2\\right)}-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-2\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-2\\right)}-\\boldsymbol{x}^{\\left(t-3\\right)}\\right)-\\boldsymbol{x}^{\\left(t-2\\right)}\\right)\\right)=$$ \n",
    "\n",
    "$${x}^{\\left(t + 1\\right)}={x}^{\\left(t\\right)}-\\mu\\nabla f\\left({x}^{\\left(t\\right)}\\right)+\\beta\\left(-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-1\\right)}\\right)+\\beta\\left(-\\mu\\nabla f\\left(\\boldsymbol{x}^{\\left(t-2\\right)}\\right)+\\beta\\left(\\boldsymbol{x}^{\\left(t-2\\right)}-\\boldsymbol{x}^{\\left(t-3\\right)}\\right)\\right)\\right)=$$ \n",
    "\n",
    "$$\\boldsymbol{x}^{\\left(t+1\\right)}=\\boldsymbol{x}^{\\left(t\\right)}-\\mu\\sum_{s=0}^{t}\\beta^{t-s}\\nabla f\\left(\\boldsymbol{x}^{\\left(s\\right)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BiAINIpX-xM"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "### 1.3.4\n",
    "1. Implement the following function:\n",
    "\n",
    "```python\n",
    "def ApplyGradientDescentMomentum(f, Df, X0, lr, β, ε):\n",
    "    '''\n",
    "    Apply gradient descent with momentum.\n",
    "    Args:\n",
    "        f  - Objective function\n",
    "        Df - Objective gradient\n",
    "        X0 - Initial point\n",
    "        lr - Learning rate (step size)\n",
    "        β  - Momentum decaying factor\n",
    "        ε  - Stopping criterion value\n",
    "    Output:\n",
    "        X  - The converged X\n",
    "    Stopping criterion:\n",
    "        Stop iterate when f(X) < ε\n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "2. Set (the same) reasonable starting point $\\boldsymbol{X}^{\\left(0\\right)}$.\n",
    "3. Find $\\boldsymbol{X}$ such that $f\\left(\\boldsymbol{X}\\right)<\\epsilon$ where $\\epsilon=10^{-4}$.\n",
    "    * Time your function.\n",
    "    * Plot the final obtained $\\boldsymbol{X}$. \n",
    "    * Compare the run-time performance with and without momentum.  \n",
    "      (play with the hyper-parameters).\n",
    "        * **2% Bonus:**  \n",
    "            Plot the objective value $f(\\boldsymbol{X})$ as a function of the iterations.  \n",
    "            You may need to slightly modify your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7_J-BHOX-xO",
    "outputId": "0703e68d-c422-4ba9-ad02-de1b953703ee"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "def ApplyGradientDescentMomentum(f, Df, X0, lr, β, ε):\n",
    "    '''\n",
    "    Apply gradient descent with momentum.\n",
    "    Args:\n",
    "        f  - Objective function\n",
    "        Df - Objective gradient\n",
    "        X0 - Initial point\n",
    "        lr - Learning rate (step size)\n",
    "        β  - Momentum decaying factor\n",
    "        ε  - Stopping criterion value\n",
    "    Output:\n",
    "        X  - The converged X\n",
    "    Stopping criterion:\n",
    "        Stop iterate when f(X) < ε\n",
    "    '''\n",
    "    list_fx = list()\n",
    "    cur_x = X0\n",
    "    count = 0\n",
    "    x_minus_2 = cur_x - lr * Df(cur_x)\n",
    "    cur_x = x_minus_2 - lr * Df(x_minus_2)\n",
    "    \n",
    "    previous_step_size = np.linalg.norm(Y- H@cur_x@H.T)**2\n",
    "\n",
    "    \n",
    "    while previous_step_size > ε and numberOfItr <MAX_ITERATIONS:\n",
    "        \n",
    "        prev_x = cur_x.copy()\n",
    "        cur_x +=  - lr * Df(prev_x) - β * (prev_x - x_minus_2)\n",
    "        x_minus_2 = prev_x.copy()\n",
    "        \n",
    "        previous_step_size = np.linalg.norm(Y- H@cur_x@H.T)**2\n",
    "        list_fx.append(previous_step_size)\n",
    "\n",
    "        count += 1\n",
    "    return cur_x, list_fx\n",
    "\n",
    "\n",
    "X0 = Y.copy()\n",
    "lr = 0.1\n",
    "ε =math.pow(10,-4)\n",
    "β = 0.9\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "new_x, list_fx = ApplyGradientDescentMomentum(f, Df, X0, lr, β, ε)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "GradientDescentMomentum_time = end - start\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(Y,     cmap='gray')\n",
    "ax[1].imshow(new_x, cmap='gray')\n",
    "ax[0].set_title('$Y = H X H^T$')\n",
    "ax[1].set_title('$X^{\\star}$')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOKJDKtkNc7p",
    "outputId": "448aa042-cfef-42bd-ac34-161846963666"
   },
   "outputs": [],
   "source": [
    "print(' ')\n",
    "\n",
    "plt.plot(np.array(list_fx)) \n",
    "plt.title('f(x)  as a function of the iterations:')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('error')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eN9Q-HENc7p",
    "outputId": "aab71e49-472b-42aa-ad02-f20937799de9"
   },
   "outputs": [],
   "source": [
    "print('GradientDescent_time', GradientDescent_time)\n",
    "print('GradientDescentMomentum_time', GradientDescentMomentum_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tgEMCmjX-xP"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "# 2. Clustering\n",
    "## 2.1 K-Means\n",
    "### 2.1.3\n",
    "Implement the K-Means algorithm.\n",
    "```python\n",
    "def KMeans(mX, K):\n",
    "    '''\n",
    "    Apply K-Means\n",
    "    Args:\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "        K    - Number of clusters, positive integer\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to, vIdx.shape = N\n",
    "        mMu  - Clusters' centers,                          mMu.shape  = d, K\n",
    "        \n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "**Notes**:\n",
    "* The algorithm halts when no update occur.  \n",
    "* **4% Bonus**: Implement the K-Menas++ initialization: https://en.wikipedia.org/wiki/K-means%2B%2B\n",
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTa5LP2qX-xR"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import sys\n",
    "\n",
    "MAX_DIST = 1000000\n",
    "MAX_ITERATIONS = 1000000\n",
    "\n",
    "def KMeans(mX, K):\n",
    "    '''\n",
    "    Apply K-Means\n",
    "    Args:\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "        K    - Number of clusters, positive integer\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to, vIdx.shape = N\n",
    "        mMu  - Clusters' centers,                          mMu.shape  = d, K\n",
    "        \n",
    "    '''    \n",
    "    clusters_dict = initial_clusters(mX, K)\n",
    "    last_centroids = [cluster['centroid'] for i, cluster in clusters_dict.items()]\n",
    "    cur_centroids =  np.zeros_like(last_centroids)\n",
    "    \n",
    "    i = 0 \n",
    "    while not identical(last_centroids, cur_centroids):\n",
    "        i+= 1\n",
    "        \n",
    "        reset_members_lists(clusters_dict)\n",
    "        cluster_mX(clusters_dict, mX)  # indexes of the points \n",
    "        calculate_new_centroids(clusters_dict, mX)\n",
    "        last_centroids = cur_centroids.copy()\n",
    "        cur_centroids = [cluster['centroid'] for cluster in clusters_dict.values()]\n",
    "    \n",
    "    # map each x to its cluster\n",
    "    labels_list = [0]*len(mX)\n",
    "    for i, cluster in clusters_dict.items():\n",
    "        for member in cluster['members']:\n",
    "            labels_list[member] = i\n",
    "    \n",
    "    return labels_list\n",
    "\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2)**2))\n",
    "   \n",
    "# K-Menas++ initialization algorithm\n",
    "def initialize_plus(mX, k):\n",
    "   \n",
    "    centroids = []\n",
    "    mX_np = np.array(mX)\n",
    "    centroids.append(mX_np[np.random.randint(mX_np.shape[0]), :])\n",
    "    n_points = np.array(mX_np).shape[0]\n",
    "   \n",
    "    for center in range(k - 1):\n",
    "          \n",
    "        dist = []\n",
    "        for i in range(mX_np.shape[0]):\n",
    "\n",
    "            point = mX_np[i, :]\n",
    "            d = sys.maxsize\n",
    "              \n",
    "            for j in range(len(centroids)):\n",
    "                temp_dist = distance(point, centroids[j])\n",
    "                d = min(d, temp_dist)\n",
    "            dist.append(d)\n",
    "              \n",
    "        ## select data point with maximum distance as our next centroid \n",
    "        next_centroid = mX_np[np.argmax(dist), :] \n",
    "        centroids.append(next_centroid) \n",
    "\n",
    "        # Reinitialize distance array for next centroid\n",
    "        dist = np.empty(n_points)\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "def initial_clusters(mX, K):\n",
    "    '''\n",
    "    Initialize the first K clusters\n",
    "    Args:\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "        K    - Number of centroids\n",
    "    Output:\n",
    "        clusters_dict - the clusters and their members\n",
    "        \n",
    "    ''' \n",
    "    samples = initialize_plus(mX, K)\n",
    "    #samples = random.sample([x for x in mX], K)\n",
    "    clusters_dict = defaultdict(dict)\n",
    "    for i, x in enumerate(samples):\n",
    "        clusters_dict[i]['members'] = list()\n",
    "        clusters_dict[i]['centroid'] = x\n",
    "    return clusters_dict\n",
    "                \n",
    "def reset_members_lists(clusters_dict):\n",
    "    '''\n",
    "    Remove te members of each cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their members with the new members list\n",
    "        \n",
    "    '''    \n",
    "    for i, dct in clusters_dict.items():\n",
    "        dct['members'] = list()\n",
    "\n",
    "def calculate_new_centroids(clusters_dict, mX):\n",
    "    '''\n",
    "    Remove te members of each cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their members with the new centroids\n",
    "        \n",
    "    '''   \n",
    "    for cluster, cluster_dict in clusters_dict.items():\n",
    "        cluster_dict['centroid'] = [np.mean(x) for x in zip(*[mX[i] for i in cluster_dict['members']])]\n",
    "\n",
    "def cluster_mX(clusters_dict, mX):\n",
    "    '''\n",
    "    Assign each point to the relevant cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their new members\n",
    "    '''\n",
    "    centroids = [cluster['centroid'] for i, cluster in clusters_dict.items()] \n",
    "\n",
    "    max_dist = MAX_DIST\n",
    "    for j, x in enumerate(mX):\n",
    "        dists = []\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            \n",
    "            dists.append(np.sqrt(np.sum([(x[i] - centroid[i])**2   for i in range(len(x))])))\n",
    "            \n",
    "        minimal_centroid_index = dists.index(min(dists))\n",
    "        \n",
    "        clusters_dict[minimal_centroid_index]['members'].append(j)\n",
    "        \n",
    "def identical(lst_1, lst_2):\n",
    "    \n",
    "    for i in range(len(lst_1)):\n",
    "        for j in range(len(lst_1[0])):\n",
    "            if lst_1[i][j] != lst_2[i][j]:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRuNPVjxX-xU"
   },
   "source": [
    "# verify K-Means results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "X8l3TU2uX-xW",
    "outputId": "afc47d7a-bcc7-4a4e-ee9a-fa2afdf8e3cd"
   },
   "outputs": [],
   "source": [
    "mMu = np.array([[4,   4],\n",
    "                [-3, -3],\n",
    "                [-2, -8],\n",
    "                [-8, -2]])\n",
    "Ni = 150\n",
    "mX = np.row_stack([np.random.randn(Ni, 2) + vMu for vMu in mMu])\n",
    "N  = mX.shape[0]\n",
    "vC = KMeans(mX, 4)     \n",
    "\n",
    "Nc = len(set(vC))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(mX[:,0], mX[:,1], s=100, c=vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw06lG7zX-xX"
   },
   "source": [
    "### Super-pixels\n",
    "1. Load the `Fruits.jpeg` image and covert it to NumPy ndarray `mI`:\n",
    "$$\\boldsymbol{I}\\in\\mathbb{R}^{\\text{height}\\times\\text{width}\\times3}$$\n",
    "2. Use the LAB color space (instead of RGB) (`mLAB`).\n",
    "3. Create a new \"image\" `mX` such that:\n",
    "$$\\boldsymbol{X}\\in\\mathbb{R}^{\\text{height}\\times\\text{width}\\times5}$$\n",
    "where:\n",
    "    * The first 3 channels are the LAB image.\n",
    "    * The 4th channel is the $x$ position.\n",
    "    * The 5th channel is the $y$ position.\n",
    "4. Apply K-Means to the pixels of `mX` (set a reasonable $K$).\n",
    "5. Create a mask image `mMask` such that:\n",
    "$$\\boldsymbol{M}\\in\\mathbb{R}^{\\text{height}\\times\\text{width}}$$\n",
    "where each pixel in `mMask` is the cluster index of the corresponding pixel in `mI`.\n",
    "6. Plot the segmentation (Superpixels) map.\n",
    "\n",
    "**Tip:** Try different weights for the LAB image and the XY position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tLkb7XHX-xY",
    "outputId": "cfedb6f1-1807-4464-a92e-04b1f781a998"
   },
   "outputs": [],
   "source": [
    "from PIL                  import Image\n",
    "from skimage              import color\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from numpy import asarray\n",
    "from skimage import io, color\n",
    "\n",
    "\n",
    "oImage = Image.open('Fruits.jpeg')\n",
    "oImage\n",
    "print(oImage.format)\n",
    "print(oImage.size)\n",
    "print(oImage.mode)\n",
    "\n",
    "\n",
    "lab = color.rgb2lab(oImage)\n",
    "\n",
    "numpydata = asarray(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587
    },
    "id": "XYg8pyQfX-xZ",
    "outputId": "ac06f6d7-2b88-45c0-be4e-c04b8a197863"
   },
   "outputs": [],
   "source": [
    "# simple initialization results\n",
    "\n",
    "mI   = np.array(oImage)\n",
    "mLAB = color.rgb2lab(mI)\n",
    "\n",
    "new_im = np.zeros((numpydata.shape[0], numpydata.shape[1] ,5))\n",
    "im_as_lst = list()\n",
    "for i, hight in enumerate(lab):\n",
    "    for j, width in enumerate(hight):\n",
    "        new_im[i, j] = np.concatenate((width, [i, j]))\n",
    "        im_as_lst.append(new_im[i, j])\n",
    "\n",
    "mMask = KMeans(im_as_lst, 30)\n",
    "\n",
    "\n",
    "mO = mark_boundaries(mI, np.array(mMask).reshape(375, 500), color=(0,1,1))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(mO)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJ7vcyinNc7r",
    "outputId": "6b361fa9-f76d-4fac-8952-bea7737084fd"
   },
   "outputs": [],
   "source": [
    "# initialize_plus_plus results\n",
    "\n",
    "mMask = KMeans(im_as_lst, 30)\n",
    "\n",
    "mO = mark_boundaries(mI, np.array(mMask).reshape(375, 500), color=(0,1,1))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(mO)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCgrbaX3X-xb"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "## 2.2 GMM\n",
    "The GMM algorithm aims to maximize the (log) likelihood objective:\n",
    "$$\\arg\\max_{\\left\\{ \\left(w_{k},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right\\} _{k=1}^{K}}f=\\arg\\max_{\\left\\{ \\left(w_{k},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right\\} _{k=1}^{K}}\\sum_{i=1}^{N}\\log\\left(\\sum_{k=1}^{K}w_{k}\\mathcal{N}\\left(\\boldsymbol{x}_{i};\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right)$$\n",
    "### 2.2.3\n",
    "Implement the GMM algorithm.\n",
    "```python\n",
    "def GMM(mX, K, ε=1e-6, maxIter=100):\n",
    "    '''\n",
    "    Apply GMM\n",
    "    Args:\n",
    "        mX      - Input data,                   mX.shape = d, N\n",
    "        K       - Number of clusters,           positive integer\n",
    "        ε       - Stopping criterion threshold, positive real\n",
    "        maxIter - Maximum number of iterations, positive integer\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to, vIdx.shape = N\n",
    "        vW   - The weight of each Gaussian,                vW.shape   = K\n",
    "        mMu  - Centers,                                    mMu.shape  = d, K\n",
    "        mSig - Covariance,                                 mSig.shape = d, d, K\n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "**Notes**:\n",
    "* Stopping criteria:\n",
    "    * The objective $f$ has changed less than $\\epsilon$.\n",
    "    * Maximum number of iteration.\n",
    "* The index $s$ of the point $\\boldsymbol{x}_i$ is defined by:\n",
    "$$\\boldsymbol{x}_{i}\\in\\mathcal{D}_{s} \\iff s=\\arg\\max_{k}w_{k}\\mathcal{N}\\left(\\boldsymbol{x}_{i},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)$$\n",
    "* **2% Bonus:** Use [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) and avoid `for` loops in step II. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XouZC1y5X-xc"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "Consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "U2UCJrHSX-xd",
    "outputId": "ff7872c6-9650-457d-c7dd-06152d235000"
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy as sp\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N1    = 250\n",
    "N2    = 150\n",
    "N3    = 200\n",
    "\n",
    "vMu1  = np.array([0, 0])\n",
    "vMu2  = np.array([2, 0.5])\n",
    "vMu3  = np.array([4, 1])\n",
    "\n",
    "mSig1 = .5 * np.array([[1.00, 1.25],\n",
    "                       [1.25, 2.00]])\n",
    "\n",
    "mSig2 = .5 * np.array([[ 1.00, -1.25],\n",
    "                       [-1.25,  2.00]])\n",
    "\n",
    "mSig3 = .5 * np.array([[1.00, 1.25],\n",
    "                       [1.25, 2.00]])\n",
    "\n",
    "mX1 = sp.stats.multivariate_normal.rvs(mean=vMu1, cov=mSig1, size=N1)\n",
    "mX2 = sp.stats.multivariate_normal.rvs(mean=vMu2, cov=mSig2, size=N2)\n",
    "mX3 = sp.stats.multivariate_normal.rvs(mean=vMu3, cov=mSig3, size=N3)\n",
    "mX1  = np.r_[mX1, mX2, mX3].T\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(mX1[0,:], mX1[1,:], s=50, edgecolors='k', color='b')\n",
    "plt.axis('equal')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft-pt9yvX-xf"
   },
   "outputs": [],
   "source": [
    "def init_mus(k):\n",
    "    \n",
    "    return np.random.uniform(low=-2, high=2, size=(k,))\n",
    "\n",
    "#-- w * N(x; mu, sig^2):\n",
    "def Pz(x, w, vMu, mSig):\n",
    "    return w * sp.stats.multivariate_normal.pdf(x, vMu, mSig)\n",
    "\n",
    "\n",
    "def GMM_EM(mX, K, ε=1e-6, maxIter=100):\n",
    "    \n",
    "    d = len(mX[0])\n",
    "    w_list = [1/K for i in range(K)]\n",
    "    mu_list = [init_mus(d) for i in range(K)]\n",
    "    sigma_list = [np.eye(d) for i in range(K)]\n",
    "    \n",
    "    vW     = np.array(w_list)\n",
    "    mMu   = np.array(mu_list).T\n",
    "    mSig   = np.dstack((w for w in sigma_list))\n",
    "        \n",
    "    N = len(mX)\n",
    "    ii = 0\n",
    "    loss = MAX_ITERATIONS\n",
    "    prev_loss = 0\n",
    "    loss_2 = 0\n",
    "    while ii < maxIter and abs(loss - prev_loss) > ε: \n",
    "        \n",
    "        #-- Step 1, estimate probabilites:\n",
    "        Px = list()\n",
    "        Ni = list()\n",
    "        for i in range(len(vW)): \n",
    "            Px.append(Pz(mX, vW[i], mMu[:,i], mSig[:,:,i]))\n",
    "        \n",
    "        \n",
    "        vSum = sum([Px[i] for i in range(len(vW))])\n",
    "        for i in range(len(vW)):\n",
    "            Px[i]  = Px[i] / vSum\n",
    "\n",
    "\n",
    "        #-- Step 2, estimate params:\n",
    "        for i in range(len(vW)):\n",
    "            Ni.append(np.sum(Px[i]))\n",
    "            vW[i] = Ni[i]/ N\n",
    "            mMu[:,i]    = np.sum(Px[i][:,None] * mX, axis=0) / Ni[i]\n",
    "            mSig[:,:,i] = (Px[i][:,None] * (mX - mMu[:,i])).T @ (mX - mMu[:,i]) / Ni[i]\n",
    "            \n",
    "        ii += 1\n",
    "        prev_loss = loss\n",
    "        labels_lst = [np.array([Px[j][i] for j in range(K)]).argmax(axis=0) for i in range(len(mX))]\n",
    "        loss = np.log2(sum([Px[label][i] for i, label in enumerate(labels_lst)]))\n",
    "            \n",
    "    return labels_lst, vW, mMu, mSig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2eRBkN6X-xg"
   },
   "source": [
    "### 2.2.4 Compare K-Means to GMM \n",
    "* Set $K=3$ and apply K-Means and GMM to the above dataset.\n",
    "* Repeat several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7LtMPXeX-xh",
    "outputId": "da533f37-d2f7-4ef3-a2e1-988d0d673d5e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import numpy as np\n",
    "mMu = np.array([[4,   4],\n",
    "                [-3, -3],\n",
    "                [-2, -8],\n",
    "                [-8, -2]])\n",
    "Ni = 150\n",
    "mX = np.row_stack([np.random.randn(Ni, 2) + vMu for vMu in mMu])\n",
    "N  = mX.shape[0]\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    GMM_EM_vC, lW, lMu, lSig = GMM_EM(mX1.T, 3)\n",
    "    KMeans_vC = KMeans(mX1.T, 3) \n",
    "    \n",
    "    Nc = 3\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n",
    "    plt.figure()\n",
    "    \n",
    "    axes[0].scatter(mX1.T[:,0], mX1.T[:,1], s=100, c=GMM_EM_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[0].set_title('GMM')\n",
    "    axes[1].scatter(mX1.T[:,0], mX1.T[:,1], s=100, c=KMeans_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[1].set_title('KMeans')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z7DTAp5X-xk"
   },
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "## 2.4 DBSCAN\n",
    "### 2.4.1\n",
    "Implement the DBSCAN algorithm.\n",
    "```python\n",
    "def DBSCAN(mX, Z, r):\n",
    "    '''\n",
    "    Apply DBSCAN\n",
    "    Args:\n",
    "        mX   - Input data,                                   mX.shape = d, N\n",
    "        Z    - Number of points required to be a core point, positive integer\n",
    "        r    - Neighborhood radius,                          positive real.\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to,   vIdx.shape = N\n",
    "    '''\n",
    "    pass\n",
    "```\n",
    "**Notes**:\n",
    "* Noise points should have index `-1`.\n",
    "* Implement an auxiliary function to compute connected components (using BFS or DFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsW8kbN-X-xk"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "\n",
    "def get_dist(point_i, point_j):\n",
    "    '''\n",
    "    Get auclidian distance \n",
    "    Args:\n",
    "        point_i\n",
    "        point_j    \n",
    "    Output:\n",
    "        dist\n",
    "    '''\n",
    "    return np.sqrt(np.sum([(point_i[i] - point_j[i])**2   for i in range(len(point_i))]))\n",
    "\n",
    "def get_distances_table(mX, r, Z):\n",
    "    '''\n",
    "    Get auclidian distance between the points\n",
    "    Get core points and their \"neighbors\"\n",
    "    Args:\n",
    "        mX   - Input data,                mX.shape = d, N.\n",
    "        r    - Neighborhood radius,       positive real.  \n",
    "    Output:\n",
    "        full_distances_table - distance between each point\n",
    "        core_points_dict - core points and their \"neighbors\"\n",
    "    '''\n",
    "    core_points_dict = defaultdict(list)\n",
    "    full_distances_table = defaultdict(dict)\n",
    "    \n",
    "    for i, point_i in enumerate(mX):\n",
    "        \n",
    "        for j in range(i+1, len(mX)):\n",
    "            \n",
    "            point_j = mX[j]\n",
    "            dist = get_dist(point_i, point_j)\n",
    "\n",
    "            full_distances_table[i][j] = dist\n",
    "            full_distances_table[j][i] = dist\n",
    "            if dist < r:\n",
    "                core_points_dict[i].append(j)\n",
    "                core_points_dict[j].append(i)\n",
    "                \n",
    "    core_points_dict = {i:points for i, points in core_points_dict.items() if len(points) > Z}\n",
    "\n",
    "    return core_points_dict, full_distances_table\n",
    "\n",
    "def get_connected_components(G):\n",
    "    '''\n",
    "    Get all connected components using BFS algorithm\n",
    "    Args:\n",
    "        G   - a graph - dict\n",
    "    Output:\n",
    "        connected_components - dict - each components with their members\n",
    "\n",
    "    '''\n",
    "    def find_root(node,root):\n",
    "        while node != root[node][0]:\n",
    "            node = root[node][0]\n",
    "        return (node,root[node][1])\n",
    "    cur_root = {} \n",
    "    for node in G.keys():\n",
    "        cur_root[node] = (node,0)  \n",
    "    for node_i in G: \n",
    "        for node_j in G[node_i]: \n",
    "            (root_i,depth_i) = find_root(node_i,cur_root) \n",
    "            (root_j,depth_j) = find_root(node_j,cur_root) \n",
    "            if root_i != root_j: \n",
    "                min_root = root_i\n",
    "                max_root = root_j \n",
    "                if  depth_i > depth_j: \n",
    "                    min_root = root_j\n",
    "                    max_root = root_i\n",
    "                cur_root[max_root] = (max_root,max(cur_root[min_root][1]+1,cur_root[max_root][1]))\n",
    "                cur_root[min_root] = (cur_root[max_root][0],-1) \n",
    "    connected_components = {}\n",
    "    for node_i in G: \n",
    "        if cur_root[node_i][0] == node_i:\n",
    "            connected_components[node_i] = []\n",
    "    for node_i in G: \n",
    "        connected_components[find_root(node_i,cur_root)[0]].append(node_i) \n",
    "    return connected_components\n",
    "\n",
    "def get_G(mX, r, core_points_dict):\n",
    "    '''\n",
    "    Set graph G(C, E) whereas x_i, x_j (of mX) are connected if dist(x_i, x_j) < r\n",
    "    Get also the boundery points list ( all the points within radius r from thier core point)\n",
    "    Args:\n",
    "        mX   - Input data,                                   mX.shape = d, N\n",
    "        r    - Neighborhood radius,                          positive real.\n",
    "        core_points - points that at least Z points are within radius r from it \n",
    "        core_points_dict - \n",
    "    Output:\n",
    "        G   - the graph\n",
    "        myToRet - distance between each point\n",
    "\n",
    "    '''\n",
    "    boundery_points = []\n",
    "    G = defaultdict(list)\n",
    "    core_points = list(core_points_dict.keys())\n",
    "    for i, i_core_point in enumerate(core_points):\n",
    "        boundery_points.extend(core_points_dict[i_core_point])\n",
    "        for j in range(i+1, len(core_points)):\n",
    "            j_core_point = core_points[j]\n",
    "\n",
    "            if get_dist(mX[i_core_point], mX[j_core_point]) < r:\n",
    "                G[i_core_point].append(j_core_point)\n",
    "                G[j_core_point].append(i_core_point)\n",
    "    return G, boundery_points\n",
    "\n",
    "def DBSCAN(mX, Z, r):\n",
    "    '''\n",
    "    Apply DBSCAN\n",
    "    Args:\n",
    "        mX   - Input data,                                   mX.shape = d, N\n",
    "        Z    - Number of points required to be a core point, positive integer\n",
    "        r    - Neighborhood radius,                          positive real.\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to,   vIdx.shape = N\n",
    "    '''\n",
    "    core_points_dict, full_distances_table = get_distances_table(mX, r, Z)\n",
    "    core_points = list(core_points_dict.keys())\n",
    "    \n",
    "    G, boundery_points = get_G(mX, r, core_points_dict)\n",
    "    \n",
    "    boundery_points = set(boundery_points) - set(core_points)\n",
    "\n",
    "    connected_components = get_connected_components(G)\n",
    "    points_comps = defaultdict(lambda:-1) # each point assign to relevant component\n",
    "    \n",
    "    # map each core point to it's component \n",
    "    for comp, comp_lst in connected_components.items():\n",
    "        for point in comp_lst:\n",
    "            points_comps[point] = comp\n",
    "    \n",
    "    # connect boundery points to its closest component\n",
    "    for boundery_point in boundery_points:\n",
    "        min_dist_point = {'closest_point': -1, 'dist': -1}\n",
    "        for point, point_dist in full_distances_table[boundery_point].items():\n",
    "\n",
    "            if point in boundery_points or point in core_points:\n",
    "                    if point_dist < min_dist_point['dist']:\n",
    "                        min_dist_point = {'closest_point': point, 'dist': point_dist}\n",
    "        \n",
    "        relevant_component = points_comps[min_dist_point['closest_point']] \n",
    "        points_comps[boundery_point] =  relevant_component      \n",
    "        \n",
    "        if relevant_component != -1:\n",
    "            connected_components[relevant_component].append(boundery_point)\n",
    "    \n",
    "    components = [-1] * len(mX)\n",
    "    i = 0 \n",
    "    for comp, comp_lst in connected_components.items():\n",
    "        for point in comp_lst:\n",
    "            components[point] = i\n",
    "        i+= 1\n",
    "    return components, core_points    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBYXL0-lX-xm"
   },
   "source": [
    "# verify DBSCAN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SdLW5uyX-xn",
    "outputId": "0ce761d0-8214-4c89-b8dc-c16ee88876b3"
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('font', **{'size' : 16})\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "N1     = 250\n",
    "N2     = 50\n",
    "mX1, _ = make_moons(N1, noise=.05)\n",
    "mX2, _ = make_moons(N1, noise=.05)\n",
    "mX2    = mX2 * [1, -1] + [0, 3]\n",
    "mX3    = np.random.rand(N2, 2) * [4, 5] - [1.75, 2/3]\n",
    "mX     = np.r_[mX1, mX2, mX3]\n",
    "\n",
    "vC, core_points = DBSCAN(mX, r=.2, Z=4)     \n",
    "Nc = len(set(vC))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(mX[:,0], mX[:,1], s=100, c=vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2OV6a9MX-xp"
   },
   "source": [
    "### 2.4.2 Comparison\n",
    "1. Consider the following datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "0Sx9pd7kX-xr",
    "outputId": "da5e4b14-ce59-4aac-ce70-5b2865f6dd1f"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N   = 1000\n",
    "A   = np.array([[0.6, -0.6],\n",
    "                [-0.4, 0.8]]);\n",
    "mX1 = datasets.make_circles(n_samples=N, noise=0.02)      [0]\n",
    "mX2 = datasets.make_moons  (n_samples=N, noise=0.05)      [0]\n",
    "mX3 = datasets.make_blobs  (n_samples=N, random_state=170)[0] @ A\n",
    "mX4 = datasets.make_blobs  (n_samples=N, random_state=170, cluster_std=[.8, 2, .4])[0] \n",
    "mX5 = np.load('clusterable_data.npy')\n",
    "\n",
    "lDatasets = [mX1, mX2, mX3, mX4, mX5]\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(18, 4))\n",
    "for ii, ax in enumerate(fig.axes):\n",
    "    mXi = lDatasets[ii]\n",
    "    ax.scatter(*mXi.T, c='lime', s=15, edgecolor='k')\n",
    "    ax.axis('equal')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs5eZJLtX-x0"
   },
   "source": [
    "2. Apply and compare your clustering methods to these datasets.\n",
    "    * **2% Bonus:**  \n",
    "    Add to your comparison the hierarchical and HDBSCAN methods.  \n",
    "    You may use existing libraries for these methods.\n",
    "    \n",
    "**Note:** The datasets are transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9sN8-s6OJAe",
    "outputId": "f52a5b73-e5d1-478e-a7b1-c5788e6c56c2"
   },
   "outputs": [],
   "source": [
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tnAmNpJ-X-x1",
    "outputId": "5f463a64-5a98-42cf-83db-efab51403bb8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import hdbscan\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "hyper_parameters =  {0: {'r': .09, 'Z': 3, 'k': 2},\n",
    "                     1: {'r': .2, 'Z': 4, 'k': 2},\n",
    "                     2: {'r': .35, 'Z': 12, 'k': 3},\n",
    "                     3: {'r': 1.2, 'Z': 12, 'k': 3},\n",
    "                     4: {'r': 0.04, 'Z': 10, 'k': 3},\n",
    "    \n",
    "                    }\n",
    "\n",
    "\n",
    "for i, mX in enumerate(lDatasets):\n",
    "\n",
    "    KMeans_vC = KMeans(mX, hyper_parameters[i]['k'])\n",
    "    GMM_EM_vC, lW, lMu, lSig = GMM_EM(mX, hyper_parameters[i]['k'])\n",
    "    DBSCAN_vC, core_points = DBSCAN(mX, Z=hyper_parameters[i]['Z'], r=hyper_parameters[i]['r'])\n",
    "    HDBSCAN_vC = hdbscan.HDBSCAN(min_cluster_size=hyper_parameters[i]['Z'], cluster_selection_epsilon=hyper_parameters[i]['r']).fit(mX).labels_\n",
    "    hierarchical_vC = AgglomerativeClustering(n_clusters=hyper_parameters[i]['k']).fit(mX).labels_\n",
    "\n",
    "    Nc = hyper_parameters[i]['k'] + 1\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,5))\n",
    "    plt.figure()\n",
    "    \n",
    "    axes[0].scatter(mX[:,0], mX[:,1], s=100, c=KMeans_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[0].set_title('KMeans')\n",
    "    axes[1].scatter(mX[:,0], mX[:,1], s=100, c=GMM_EM_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[1].set_title('GMM')\n",
    "    axes[2].scatter(mX[:,0], mX[:,1], s=100, c=DBSCAN_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[2].set_title('DBSCAN')\n",
    "    axes[3].scatter(mX[:,0], mX[:,1], s=100, c=HDBSCAN_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[3].set_title('HDBSCAN')\n",
    "    axes[4].scatter(mX[:,0], mX[:,1], s=100, c=hierarchical_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[4].set_title('Hierarchical')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProblemSet1_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
