{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "# K-mean - objective\n",
    "* $\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2}$<br>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1\n",
    "\n",
    "* 1.1.1 Show that the following two objectives are equivalent to the K-Means objective, as a sole function of the clusters.\n",
    "* 1.1.2 Show that the following two objectives are equivalent to the K-Means objective, as a sole function of the centroids.\n",
    " \n",
    "# 1.1.1\n",
    "* needed to show that:\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} = \\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} {x_j}\\ \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x_j}\\right\\|_{2}^{2}$   \n",
    "* note that :<br> \n",
    "$\\mu_{k} = \\frac{1}{|D_{k}|}\\sum_{\\boldsymbol{x}_{j} \\in \\mathcal{D}_{k}}\\left({x}_{j})\\right)$, where $D_{k}$ is the amount of point in the centroid \n",
    "* we will start from the left ellement:\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} = \\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\frac{1}{|D_{k}|}\\sum_{\\boldsymbol{x}_{j} \\in \\mathcal{D}_{k}}\\left({x}_{j})\\right)\\right\\|_{2}^{2} = \\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} {x_j}\\ \\in \\mathcal{D}_{k}}\\left\\|(\\boldsymbol{x}_{i}-\\boldsymbol{x_j)/D_k}\\right\\|_{2}^{2}$\n",
    "* note that it is equal to minimize without $\\frac{something}{D_k}$ \n",
    "* we show that:\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} = \\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} {x_j}\\ \\in \\mathcal{D}_{k}}\\left\\|(\\boldsymbol{x}_{i}-\\boldsymbol{x_j)}\\right\\|_{2}^{2}$\n",
    "\n",
    "# may be remove\n",
    "\n",
    "* for each cluster we find the distance between all pairs in the cluser and we minimize the mean of those distance of each cluster, by doing that we will find for each point the closeset centeriod and the that is what the K means does\n",
    "\n",
    "# 1.1.2\n",
    "* needed to show that:\n",
    "$\\arg \\min _{\\left\\{\\mathcal{D}_{k}\\right\\},\\left\\{\\mu_{k}\\right\\}} \\sum_{k=1}^{K} \\sum_{\\boldsymbol{x}_{i} \\in \\mathcal{D}_{k}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} = \\arg \\min_{\\left\\{\\mu_{k}\\right\\}} \\sum_{i=1}^{N}min_k\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_{k}\\right\\|_{2}^{2} $   \n",
    "\n",
    "* we find the min of each $x_i$ from from all the centroid and assign that $x_i$ to the cluser of the closet centroid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 \n",
    "\n",
    "* neede to aprove/disaprove the fllowing fact K-Means algorithm always converges to a global minimum.\n",
    "\n",
    "# answer\n",
    "* K-mean is not converges always to local minumum because its depand on initial centorid intialization.\n",
    "* in class we show exaple that each time the clustring of k-mean return different results.\n",
    "\n",
    "# Can add his example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "np.random.seed(1)\n",
    "mMu = np.array([[4,   4],\n",
    "                [-3, -3],\n",
    "                [-2, -8],\n",
    "                [-8, -2]])\n",
    "Ni = 150\n",
    "mX = np.concatenate([np.random.randn(Ni, 2) + vMu for vMu in mMu])\n",
    "N  = mX.shape[0]\n",
    "mMu0 = mX[[1, 152, 153, 3],:] #-- initial means\n",
    "mMu0 = mX[[1, 152, 153, 3],:] #-- initial means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K       = 4\n",
    "oKMeans = KMeans(n_clusters=K, n_init=1, init='random').fit(mX)\n",
    "# oKMeans = KMeans(n_clusters=K).fit(mX) #-- use default for stable results\n",
    "vIdx    = oKMeans.predict(mX)\n",
    "mMu     = oKMeans.cluster_centers_\n",
    "vor     = Voronoi(mMu)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 4))\n",
    "voronoi_plot_2d(vor, ax=ax, show_points=False, line_width=2, show_vertices=False)\n",
    "plt.scatter    (mX [:,0], mX [:,1], s=50, c=vIdx, edgecolor='k')\n",
    "plt.plot       (mMu[:,0], mMu[:,1], '.r', markersize=20)\n",
    "plt.axis       ('equal')\n",
    "plt.axis       ([-12, 8, -12, 8])\n",
    "plt.show       ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - GMM\n",
    "we know that:\n",
    "* let $\\underline{X} \\sim N_{d}(\\underline{\\mu_x}, \\underline{\\underline{\\Sigma_x}})$  be a Gaussian random vector.\n",
    "* let $\\underline{Y} = a\\underline{X}+\\underline{b}$  be a Gaussian random vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance# 2.1\n",
    "what is the PDF of $\\underline{Y}$ ($f_y$)?\n",
    "* we know the pdf of X <br>\n",
    "$f_{X}(\\underline{x})=\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\underline{\\underline{\\Sigma_x}}|}} \\exp \\left(-\\frac{1}{2}(\\underline{x}-\\underline{\\mu_x})^{T} \\underline{\\underline{\\Sigma_x}}^{-1}(\\underline{x}-\\underline{\\mu_x})\\right)$\n",
    "* first note that $\\underline{Y}$ is Gaussian random vector because he is linear transformation of  $\\underline{X}$ which is Gaussian random vector\n",
    "* if we will calculate the Gaussian random vector parameters we can express his PDF, therefore will calcualte $\\underline{\\mu_y}$, and $\\underline{\\underline{\\Sigma_y}}$: \n",
    "* let calculate the average of $\\underline{Y}$ <br>\n",
    "$E[\\underline{Y}] = E[a\\underline{X}+\\underline{b}] = aE[\\underline{X}]+E[\\underline{b}] = a\\underline{\\mu_x} + \\underline{b}$\n",
    "$Var[Y] = Var[a\\underline{X}+\\underline{b}] = a^2Var[\\underline{X}] = a^2\\underline{\\underline{\\Sigma_x}}$\n",
    "* The PDF of $\\underline{Y}$ <br>\n",
    "$f_{X}(\\underline{x})=\\frac{1}{\\sqrt{(2 \\pi)^{d}|a^2\\underline{\\underline{\\Sigma_x}}|}} \\exp \\left(-\\frac{1}{2}(\\underline{x}-(a\\underline{\\mu_x} + \\underline{b}))^{T} a^-2\\underline{\\underline{\\Sigma_x}}^{-1}(\\underline{x}-(a\\underline{\\mu_x} + \\underline{b}))\\right)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Covariance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n",
    "\n",
    "# needed to prove that $\\Sigma_x$ is SPSD:\n",
    "# needed to how 2 facts\n",
    "    1) prove that $\\Sigma_x$ is symmetric\n",
    "    2) prove that all eigenvalues are non negative\n",
    "\n",
    "\n",
    "\n",
    "* for a sample of vectors $x_{i}=\\left(x_{i 1}, \\ldots, x_{i k}\\right)^{\\top}$, with $i=1, \\ldots, n$, the sample mean vector is\n",
    "$$\n",
    "\\bar{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\n",
    "$$\n",
    "*  covariance matrix is :\n",
    "$$\n",
    " \\boldsymbol{\\Sigma_x}=\\mathbb{E}\\left[(\\underline{X}-\\boldsymbol{\\mu})(\\underline{X}-\\boldsymbol{\\mu})^{T}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{\\mu}\\right)\\left(x_{i}-\\bar{\\mu}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "* let show that $\\Sigma$ is Symetric: \n",
    "\n",
    "\n",
    "$$ \\boldsymbol{\\Sigma}^{T} = $$\n",
    "\n",
    "$$ \\left(\\frac{1}{n} \\sum\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\top}\\right)^{T} = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum \\left(\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\top}\\right)^{T}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum \\left(\\left(x_{i}-\\mu\\right)^{T}\\right)^{T} \\left(x_{i}-\\mu\\right)^{T}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{\\mu}\\right)\\left(x_{i}-\\bar{\\mu}\\right)^{\\top} = \\Sigma$$\n",
    "\n",
    "* let show that $\\Sigma >= 0$ (SPSD): \n",
    "# from one side\n",
    "\n",
    "    * let $v_i$ be a non trivial eigenvector ($||v_i||_2$ != 0) corresponding to $\\lambda(\\Sigma_x)  $\n",
    "\n",
    "\n",
    "\n",
    "$$ v_i^{\\top} \\Sigma_x v_i  = v_i^{\\top} \\lambda_i v_i   = \\lambda_i ||v_i||_{2}^2  -> \\in R^1  $$\n",
    "\n",
    "# from secound side we know:\n",
    "\n",
    "$$ v_i^{\\top} \\Sigma_x v_i = v_i^{\\top} E[XX^{\\top}] v_i = E[v_i^{\\top} XX^{\\top} v_i]$$\n",
    "\n",
    "* know lets asign:\n",
    "$$ Z = X^{\\top}v_i -> E[Z^2] >= 0 $$\n",
    "\n",
    "* from combing both sides:\n",
    "    1) ||v_i||_{2}^2 > 0 \n",
    "    2) $v_i^{\\top} \\Sigma_x v_i>=0$\n",
    "    3) $v_i^{\\top} \\Sigma_x v_i =  \\lambda_i ||v_i||_{2}^2 $\n",
    "        * base the choosing of the non trivial eigenvector -> $\\lambda_i>= 0 $\n",
    "        \n",
    "        \n",
    "\n",
    "$$ E\\left[\\mathbf{v}^{T}(\\mathbf{X}-\\mathbf{\\mu})(\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right] = $$\n",
    "\n",
    "\n",
    "#\n",
    "$$ E\\left[\\left((\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right)^{T}\\left((\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right)\\right] = $$\n",
    "\n",
    "(from symmetrically)\n",
    "\n",
    "$$ E\\left\\{\\left[(\\mathbf{X}-\\mathbf{\\mu})^{T} \\mathbf{v}\\right]^{2}\\right\\}  = $$\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^{n}\\left(\\left(x_{i}-\\bar{\\mu}\\right)^{\\top} v\\right)^{2} \\geq 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# needed to Prove that the complete-linkage is indeed a metric.\n",
    "remember that metric definition is as the following: \n",
    "\n",
    "A metric on a set $X$ is a function (called distance function or simply distance)\n",
    "$$\n",
    "d: X \\times X \\rightarrow[0, \\infty)\n",
    "$$\n",
    "where $[0, \\infty)$ is the set of non-negative real numbers and for all $x, y, z \\in X$, the following three axioms are satisfied:\n",
    "1. $d(x, y)=0 \\Leftrightarrow x=y$\n",
    "identity of indiscernibles\n",
    "2. $d(x, y)=d(y, x) \\quad$ symmetry\n",
    "3. $d(x, y) \\leq d(x, z)+d(z, y) \\quad$ triangle inequality\n",
    "\n",
    "* given objective:\n",
    "\n",
    "  \n",
    "     $d_{complete-link}^2(C1,C2) = \\begin{cases}\n",
    "       \\text{0,} &\\quad\\text{ C1=C2} \\\\\n",
    "       \\text{$Max_{x_i \\in C1, x_j \\in C2}||x_i-x_j ||$ ,} &\\quad\\text{  else} \\\\ \n",
    "     \\end{cases}$\n",
    "\n",
    "\n",
    "# Axiom 1 \n",
    "\n",
    "if\n",
    "$ C_{1} == C_{2} $  => $ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right) = 0 $\n",
    "\n",
    "by definition \n",
    "\n",
    "$ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right) = 0 => C_{1} == C_{2} $\n",
    "\n",
    "\n",
    "# Axiom 2\n",
    "\n",
    "$$ d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{1}, \\mathcal{C}_{2}\\right)  =  $$\n",
    "$$\\max _{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}}\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right\\|_{2}^{2} = $$\n",
    "\n",
    "\n",
    "$$ \\max _{\\boldsymbol{x}_{i} \\in \\mathcal{C}_{1}, \\boldsymbol{x}_{j} \\in \\mathcal{C}_{2}}\\left\\|\\boldsymbol{x}_{j}-\\boldsymbol{x}_{i}\\right\\|_{2}^{2} = $$ \n",
    "\n",
    "$$d_{\\text {complete-link }}^{2}\\left(\\mathcal{C}_{2}, \\mathcal{C}_{1}\\right)$$\n",
    "\n",
    "# Axiom 3\n",
    "\n",
    "We know that norm is satisfied Triangle inequality \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 - Lance-Williams (Bouns)\n",
    "# Prove that <br>\n",
    "$ D_{12,3}\\sim = d_{single-link}(C1 \\cup C2,C3)$\n",
    "# In words, show that the Lance-Williams algorithm is correct for the single-linkage dissimilarity.\n",
    "\n",
    "* 11\n",
    "* 22\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Unsupervised Learning Methods </center>\n",
    "## <center> Problem Set 2 </center>\n",
    "### <center> Clustering </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:03:47.439390Z",
     "start_time": "2022-03-20T14:03:47.038026Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 K-Means\n",
    "### 1.3 Implementation\n",
    "Implement the K-Means algorithm.\n",
    "```python\n",
    "def KMeans(mX, K):\n",
    "```\n",
    "**Notes**:\n",
    "* The algorithm halts when no update occur.  \n",
    "* Implement the [K-Means++](https://en.wikipedia.org/wiki/K-means%2B%2B) initialization.  \n",
    "Consider writing the K-Means++ initialization as a separate function.\n",
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:09:41.425165Z",
     "start_time": "2022-03-20T14:09:41.421162Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import difflib\n",
    "import sys\n",
    "\n",
    "MAX_DIST = 1000\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "\n",
    "def KMeans(mX, K):\n",
    "    '''\n",
    "    Apply K-Means\n",
    "    Args:\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "        K    - Number of clusters, positive integer\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to, vIdx.shape = N\n",
    "        mMu  - Clusters' centers,                          mMu.shape  = d, K\n",
    "        \n",
    "    '''    \n",
    "    clusters_dict = initial_clusters(mX, K)\n",
    "    last_centroids = [cluster['centroid'] for i, cluster in clusters_dict.items()]\n",
    "    cur_centroids =  np.zeros_like(last_centroids)\n",
    "    \n",
    "    i = 0 \n",
    "    while not identical(last_centroids, cur_centroids):\n",
    "        #print(i)\n",
    "        i+= 1\n",
    "        if i > MAX_ITERATIONS:\n",
    "            break\n",
    "        reset_members_lists(clusters_dict)\n",
    "        cluster_mX(clusters_dict, mX)  # indexes of the points \n",
    "        calculate_new_centroids(clusters_dict, mX)\n",
    "        last_centroids = cur_centroids.copy()\n",
    "        cur_centroids = [cluster['centroid'] for cluster in clusters_dict.values()]\n",
    "    \n",
    "    # map each x to its cluster\n",
    "    labels_list = [0]*len(mX)\n",
    "    for i, cluster in clusters_dict.items():\n",
    "        for member in cluster['members']:\n",
    "            labels_list[member] = i\n",
    "    \n",
    "    return labels_list\n",
    "\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2)**2))\n",
    "   \n",
    "# K-Menas++ initialization algorithm\n",
    "def initialize_plus(mX, k):\n",
    "   \n",
    "    centroids = []\n",
    "    mX_np = np.array(mX)\n",
    "    centroids.append(mX_np[np.random.randint(mX_np.shape[0]), :])\n",
    "    n_points = np.array(mX_np).shape[0]\n",
    "   \n",
    "    for center in range(k - 1):\n",
    "          \n",
    "        dist = []\n",
    "        for i in range(mX_np.shape[0]):\n",
    "\n",
    "            point = mX_np[i, :]\n",
    "            d = sys.maxsize\n",
    "              \n",
    "            for j in range(len(centroids)):\n",
    "                temp_dist = distance(point, centroids[j])\n",
    "                d = min(d, temp_dist)\n",
    "            dist.append(d)\n",
    "              \n",
    "        ## select data point with maximum distance as our next centroid \n",
    "        next_centroid = mX_np[np.argmax(dist), :] \n",
    "        centroids.append(next_centroid) \n",
    "\n",
    "        # Reinitialize distance array for next centroid\n",
    "        dist = np.empty(n_points)\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "def initial_clusters(mX, K):\n",
    "    '''\n",
    "    Initialize the first K clusters\n",
    "    Args:\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "        K    - Number of centroids\n",
    "    Output:\n",
    "        clusters_dict - the clusters and their members\n",
    "        \n",
    "    ''' \n",
    "    samples = initialize_plus(mX, K)\n",
    "    #samples = random.sample([x for x in mX], K)\n",
    "    clusters_dict = defaultdict(dict)\n",
    "    for i, x in enumerate(samples):\n",
    "        clusters_dict[i]['members'] = list()\n",
    "        clusters_dict[i]['centroid'] = x\n",
    "    return clusters_dict\n",
    "                \n",
    "def reset_members_lists(clusters_dict):\n",
    "    '''\n",
    "    Remove te members of each cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their members with the new members list\n",
    "        \n",
    "    '''    \n",
    "    for i, dct in clusters_dict.items():\n",
    "        dct['members'] = list()\n",
    "\n",
    "def calculate_new_centroids(clusters_dict, mX):\n",
    "    '''\n",
    "    Remove te members of each cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their members with the new centroids\n",
    "        \n",
    "    '''   \n",
    "    for cluster, cluster_dict in clusters_dict.items():\n",
    "        cluster_dict['centroid'] = [np.mean(x) for x in zip(*[mX[i] for i in cluster_dict['members']])]\n",
    "\n",
    "def cluster_mX(clusters_dict, mX):\n",
    "    '''\n",
    "    Assign each point to the relevant cluster\n",
    "    Args:\n",
    "        clusters_dict - the clusters and their members\n",
    "        mX   - Input data,         mX.shape = d, N\n",
    "    Output:\n",
    "        clusters_dict - the updated dict of clusters and their new members\n",
    "    '''\n",
    "    centroids = [cluster['centroid'] for i, cluster in clusters_dict.items()] \n",
    "\n",
    "    max_dist = MAX_DIST\n",
    "    for j, x in enumerate(mX):\n",
    "        dists = []\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            \n",
    "            dists.append(np.sqrt(np.sum([(x[i] - centroid[i])**2   for i in range(len(x))])))\n",
    "            \n",
    "        minimal_centroid_index = dists.index(min(dists))\n",
    "        \n",
    "        clusters_dict[minimal_centroid_index]['members'].append(j)\n",
    "        \n",
    "def identical(lst_1, lst_2):\n",
    "    max_diff = 1e-0\n",
    "    for i in range(len(lst_1)):\n",
    "        for j in range(len(lst_1[0])):\n",
    "            #print(lst_1[i][j] - lst_2[i][j])\n",
    "            if np.abs(lst_1[i][j] - lst_2[i][j]) <= max_diff :\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Super-pixels\n",
    "1. Load the `Fruits.jpeg` image and covert it to NumPy ndarray `mI`:\n",
    "$$\\boldsymbol{I}\\in\\mathbb{R}^{\\text{height}\\times\\text{width}\\times3}$$\n",
    "2. Use the LAB color space (instead of RGB) (`mLAB`).\n",
    "3. Create a new \"image\" `mX` such that:\n",
    "$$\\boldsymbol{X}\\in\\mathbb{R}^{\\text{height}\\times\\text{width}\\times5}$$\n",
    "where:\n",
    "    * The first 3 channels are the LAB image.\n",
    "    * The 4th channel is the $x$ position.\n",
    "    * The 5th channel is the $y$ position.\n",
    "4. Apply K-Means to the pixels of `mX` (set a reasonable $K$).  \n",
    "    You may use your own implementation or sk-learn implementation.\n",
    "5. Create a mask image `mMask` such that:\n",
    "$$\\boldsymbol{M}\\in\\mathbb{N}_0^{\\text{height}\\times\\text{width}}$$\n",
    "where each pixel in `mMask` is the cluster index of the corresponding pixel in `mI`.\n",
    "6. Plot the segmentation (Superpixels) map.\n",
    "\n",
    "**Tips:**\n",
    "* To apply K-means you first need to `reshape` the 3D matrix (tensor) `mX`.\n",
    "* Try different weights for the color space (LAB) and the spatial space (XY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:14:24.656925Z",
     "start_time": "2022-03-20T14:14:24.604878Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL                  import Image\n",
    "from skimage              import color\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "from PIL                  import Image\n",
    "from skimage              import color\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from numpy import asarray\n",
    "from skimage import io, color\n",
    "\n",
    "\n",
    "oImage = Image.open('Fruits.jpeg')\n",
    "oImage\n",
    "print(oImage.format)\n",
    "print(oImage.size)\n",
    "print(oImage.mode)\n",
    "\n",
    "\n",
    "lab = color.rgb2lab(oImage)\n",
    "\n",
    "numpydata = asarray(lab)\n",
    "oImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:18:59.542478Z",
     "start_time": "2022-03-20T14:18:59.481423Z"
    }
   },
   "outputs": [],
   "source": [
    "mI   = np.array(oImage)\n",
    "mLAB = color.rgb2lab(mI)\n",
    "\n",
    "\n",
    "new_im = np.zeros((numpydata.shape[0], numpydata.shape[1] ,5))\n",
    "im_as_lst = list()\n",
    "for i, hight in enumerate(lab):\n",
    "    for j, width in enumerate(hight):\n",
    "        new_im[i, j] = np.concatenate((width, [i, j]))\n",
    "        im_as_lst.append(new_im[i, j])\n",
    "\n",
    "mMask = KMeans(im_as_lst, 16)\n",
    "mO = mark_boundaries(mI, np.array(mMask).reshape(375, 500), color=(0,1,1))\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(mO)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "## 2 GMM\n",
    "The GMM algorithm aims to maximize the (log) likelihood objective:\n",
    "$$\\arg\\max_{\\left\\{ \\left(w_{k},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right\\} _{k=1}^{K}}f=\\arg\\max_{\\left\\{ \\left(w_{k},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right\\} _{k=1}^{K}}\\sum_{i=1}^{N}\\log\\left(\\sum_{k=1}^{K}w_{k}\\mathcal{N}_d\\left(\\boldsymbol{x}_{i};\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)\\right)$$\n",
    "where:\n",
    "$$\\mathcal{N}_{d}\\left(\\boldsymbol{x};\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)=\\frac{1}{\\sqrt{\\left(2\\pi\\right)^{d}\\left|\\boldsymbol{\\Sigma}\\right|}}\\exp\\left(-\\frac{1}{2}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)^{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}\\right)\\right)$$\n",
    "### 2.3 Implementation\n",
    "Implement the GMM algorithm.\n",
    "```python\n",
    "def GMM(mX, K, ε=1e-6, maxIter=100):\n",
    "```\n",
    "**Notes**:\n",
    "* Stopping criteria:\n",
    "    * The objective $f$ has changed less than $\\epsilon$.\n",
    "    * Maximum number of iteration.\n",
    "* The index $s$ of the point $\\boldsymbol{x}_i$ is defined by:\n",
    "$$\\boldsymbol{x}_{i}\\in\\mathcal{D}_{s} \\iff s=\\arg\\max_{k}w_{k}\\mathcal{N}_d\\left(\\boldsymbol{x}_{i},\\boldsymbol{\\mu}_{k},\\boldsymbol{\\Sigma}_{k}\\right)$$\n",
    "* Use [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) and avoid `for` loops in Step II (parameters update). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mus(k):\n",
    "    \n",
    "    return np.random.uniform(low=-2, high=2, size=(k,))\n",
    "\n",
    "#-- w * N(x; mu, sig^2):\n",
    "def Pz(x, w, vMu, mSig):\n",
    "    return w * sp.stats.multivariate_normal.pdf(x, vMu, mSig)\n",
    "\n",
    "\n",
    "def GMM(mX, K, ε=1e-6, maxIter=100):\n",
    "    '''\n",
    "    Apply GMM\n",
    "    Args:\n",
    "        mX      - Input data,                   mX.shape = (d, N)\n",
    "        K       - Number of clusters,           positive integer\n",
    "        ε       - Stopping criterion threshold, positive real\n",
    "        maxIter - Maximum number of iterations, positive integer\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to, vIdx.shape = (N,)\n",
    "        vW   - The weight of each Gaussian,                vW.shape   = (K,)\n",
    "        mMu  - Centers,                                    mMu.shape  = (d, K)\n",
    "        mSig - Covariance,                                 mSig.shape = (d, d, K)\n",
    "    '''\n",
    "    d = len(mX[0])\n",
    "    w_list = [1/K for i in range(K)]\n",
    "    mu_list = [init_mus(d) for i in range(K)]\n",
    "    sigma_list = [np.eye(d) for i in range(K)]\n",
    "    \n",
    "    vW     = np.array(w_list)\n",
    "    mMu   = np.array(mu_list).T\n",
    "    mSig   = np.dstack((w for w in sigma_list))\n",
    "        \n",
    "    N = len(mX)\n",
    "    ii = 0\n",
    "    loss = MAX_ITERATIONS\n",
    "    prev_loss = 0\n",
    "    loss_2 = 0\n",
    "    while ii < maxIter and abs(loss - prev_loss) > ε: \n",
    "        \n",
    "        #-- Step 1, estimate probabilites:\n",
    "        Px = list()\n",
    "        Ni = list()\n",
    "        for i in range(len(vW)): \n",
    "            Px.append(Pz(mX, vW[i], mMu[:,i], mSig[:,:,i]))\n",
    "        \n",
    "        \n",
    "        vSum = sum([Px[i] for i in range(len(vW))])\n",
    "        for i in range(len(vW)):\n",
    "            Px[i]  = Px[i] / vSum\n",
    "\n",
    "\n",
    "        #-- Step 2, estimate params:\n",
    "        for i in range(len(vW)):\n",
    "            Ni.append(np.sum(Px[i]))\n",
    "            vW[i] = Ni[i]/ N\n",
    "            mMu[:,i]    = np.sum(Px[i][:,None] * mX, axis=0) / Ni[i]\n",
    "            mSig[:,:,i] = (Px[i][:,None] * (mX - mMu[:,i])).T @ (mX - mMu[:,i]) / Ni[i]\n",
    "            \n",
    "        ii += 1\n",
    "        prev_loss = loss\n",
    "        labels_lst = [np.array([Px[j][i] for j in range(K)]).argmax(axis=0) for i in range(len(mX))]\n",
    "        loss = np.log2(sum([Px[label][i] for i, label in enumerate(labels_lst)]))\n",
    "            \n",
    "    return labels_lst, vW, mMu, mSig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "Consider the following data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:57:16.067724Z",
     "start_time": "2022-03-20T14:57:15.972641Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import scipy as sp\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N1    = 250\n",
    "N2    = 150\n",
    "N3    = 200\n",
    "\n",
    "vMu1  = np.array([0, 0])\n",
    "vMu2  = np.array([2, 0.5])\n",
    "vMu3  = np.array([4, 1])\n",
    "\n",
    "mSig1 = .5 * np.array([[1.00, 1.25],\n",
    "                       [1.25, 2.00]])\n",
    "\n",
    "mSig2 = .5 * np.array([[ 1.00, -1.25],\n",
    "                       [-1.25,  2.00]])\n",
    "\n",
    "mSig3 = .5 * np.array([[1.00, 1.25],\n",
    "                       [1.25, 2.00]])\n",
    "\n",
    "mX1 = sp.stats.multivariate_normal.rvs(mean=vMu1, cov=mSig1, size=N1)\n",
    "mX2 = sp.stats.multivariate_normal.rvs(mean=vMu2, cov=mSig2, size=N2)\n",
    "mX3 = sp.stats.multivariate_normal.rvs(mean=vMu3, cov=mSig3, size=N3)\n",
    "mX1  = np.r_[mX1, mX2, mX3].T\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(mX1[0,:], mX1[1,:], s=50, edgecolors='k', color='b')\n",
    "plt.axis('equal')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare K-Means to GMM \n",
    "* Set $K=3$ and apply K-Means and GMM to the above dataset.\n",
    "* Repeat several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:59:19.073073Z",
     "start_time": "2022-03-20T14:59:19.058059Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import numpy as np\n",
    "mMu = np.array([[4,   4],\n",
    "                [-3, -3],\n",
    "                [-2, -8],\n",
    "                [-8, -2]])\n",
    "Ni = 150\n",
    "mX = np.row_stack([np.random.randn(Ni, 2) + vMu for vMu in mMu])\n",
    "N  = mX.shape[0]\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    GMM_EM_vC, lW, lMu, lSig = GMM(mX1.T, 3)\n",
    "    KMeans_vC = KMeans(mX1.T, 3) \n",
    "    \n",
    "    Nc = 3\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n",
    "    plt.figure()\n",
    "    \n",
    "    axes[0].scatter(mX1.T[:,0], mX1.T[:,1], s=100, c=GMM_EM_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[0].set_title('GMM')\n",
    "    axes[1].scatter(mX1.T[:,0], mX1.T[:,1], s=100, c=KMeans_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[1].set_title('KMeans')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------\n",
    "## 4 DBSCAN\n",
    "### 4.1 Implementation\n",
    "Implement the DBSCAN algorithm.\n",
    "```python\n",
    "def DBSCAN(mX, Z, r):\n",
    "```\n",
    "**Notes**:\n",
    "* Noise points should have index `-1`.\n",
    "* Implement an auxiliary function to compute connected components (using BFS or DFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T14:59:31.987763Z",
     "start_time": "2022-03-20T14:59:31.983759Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################################\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "\n",
    "def get_dist(point_i, point_j):\n",
    "    '''\n",
    "    Get auclidian distance \n",
    "    Args:\n",
    "        point_i\n",
    "        point_j    \n",
    "    Output:\n",
    "        dist\n",
    "    '''\n",
    "    return np.sqrt(np.sum([(point_i[i] - point_j[i])**2   for i in range(len(point_i))]))\n",
    "\n",
    "def get_distances_table(mX, r, Z):\n",
    "    '''\n",
    "    Get auclidian distance between the points\n",
    "    Get core points and their \"neighbors\"\n",
    "    Args:\n",
    "        mX   - Input data,                mX.shape = d, N.\n",
    "        r    - Neighborhood radius,       positive real.  \n",
    "    Output:\n",
    "        full_distances_table - distance between each point\n",
    "        core_points_dict - core points and their \"neighbors\"\n",
    "    '''\n",
    "    core_points_dict = defaultdict(list)\n",
    "    full_distances_table = defaultdict(dict)\n",
    "    \n",
    "    for i, point_i in enumerate(mX):\n",
    "        \n",
    "        for j in range(i+1, len(mX)):\n",
    "            \n",
    "            point_j = mX[j]\n",
    "            dist = get_dist(point_i, point_j)\n",
    "\n",
    "            full_distances_table[i][j] = dist\n",
    "            full_distances_table[j][i] = dist\n",
    "            if dist < r:\n",
    "                core_points_dict[i].append(j)\n",
    "                core_points_dict[j].append(i)\n",
    "                \n",
    "    core_points_dict = {i:points for i, points in core_points_dict.items() if len(points) > Z}\n",
    "\n",
    "    return core_points_dict, full_distances_table\n",
    "\n",
    "def get_connected_components(G):\n",
    "    '''\n",
    "    Get all connected components using BFS algorithm\n",
    "    Args:\n",
    "        G   - a graph - dict\n",
    "    Output:\n",
    "        connected_components - dict - each components with their members\n",
    "\n",
    "    '''\n",
    "    def find_root(node,root):\n",
    "        while node != root[node][0]:\n",
    "            node = root[node][0]\n",
    "        return (node,root[node][1])\n",
    "    cur_root = {} \n",
    "    for node in G.keys():\n",
    "        cur_root[node] = (node,0)  \n",
    "    for node_i in G: \n",
    "        for node_j in G[node_i]: \n",
    "            (root_i,depth_i) = find_root(node_i,cur_root) \n",
    "            (root_j,depth_j) = find_root(node_j,cur_root) \n",
    "            if root_i != root_j: \n",
    "                min_root = root_i\n",
    "                max_root = root_j \n",
    "                if  depth_i > depth_j: \n",
    "                    min_root = root_j\n",
    "                    max_root = root_i\n",
    "                cur_root[max_root] = (max_root,max(cur_root[min_root][1]+1,cur_root[max_root][1]))\n",
    "                cur_root[min_root] = (cur_root[max_root][0],-1) \n",
    "    connected_components = {}\n",
    "    for node_i in G: \n",
    "        if cur_root[node_i][0] == node_i:\n",
    "            connected_components[node_i] = []\n",
    "    for node_i in G: \n",
    "        connected_components[find_root(node_i,cur_root)[0]].append(node_i) \n",
    "    return connected_components\n",
    "\n",
    "def get_G(mX, r, core_points_dict):\n",
    "    '''\n",
    "    Set graph G(C, E) whereas x_i, x_j (of mX) are connected if dist(x_i, x_j) < r\n",
    "    Get also the boundery points list ( all the points within radius r from thier core point)\n",
    "    Args:\n",
    "        mX   - Input data,                                   mX.shape = d, N\n",
    "        r    - Neighborhood radius,                          positive real.\n",
    "        core_points - points that at least Z points are within radius r from it \n",
    "        core_points_dict - \n",
    "    Output:\n",
    "        G   - the graph\n",
    "        myToRet - distance between each point\n",
    "\n",
    "    '''\n",
    "    boundery_points = []\n",
    "    G = defaultdict(list)\n",
    "    core_points = list(core_points_dict.keys())\n",
    "    for i, i_core_point in enumerate(core_points):\n",
    "        boundery_points.extend(core_points_dict[i_core_point])\n",
    "        for j in range(i+1, len(core_points)):\n",
    "            j_core_point = core_points[j]\n",
    "\n",
    "            if get_dist(mX[i_core_point], mX[j_core_point]) < r:\n",
    "                G[i_core_point].append(j_core_point)\n",
    "                G[j_core_point].append(i_core_point)\n",
    "    return G, boundery_points\n",
    "\n",
    "\n",
    "def DBSCAN(mX, Z, r):\n",
    "    '''\n",
    "    Apply DBSCAN\n",
    "    Args:\n",
    "        mX   - Input data,                                   mX.shape = (d, N)\n",
    "        Z    - Number of points required to be a core point, positive integer\n",
    "        r    - Neighborhood radius,                          positive real.\n",
    "    Output:\n",
    "        vIdx - Index of the cluster each point belongs to,   vIdx.shape = (N,)\n",
    "    '''\n",
    "    core_points_dict, full_distances_table = get_distances_table(mX, r, Z)\n",
    "    core_points = list(core_points_dict.keys())\n",
    "    \n",
    "    G, boundery_points = get_G(mX, r, core_points_dict)\n",
    "    \n",
    "    boundery_points = set(boundery_points) - set(core_points)\n",
    "\n",
    "    connected_components = get_connected_components(G)\n",
    "    points_comps = defaultdict(lambda:-1) # each point assign to relevant component\n",
    "    \n",
    "    # map each core point to it's component \n",
    "    for comp, comp_lst in connected_components.items():\n",
    "        for point in comp_lst:\n",
    "            points_comps[point] = comp\n",
    "    \n",
    "    # connect boundery points to its closest component\n",
    "    for boundery_point in boundery_points:\n",
    "        min_dist_point = {'closest_point': -1, 'dist': -1}\n",
    "        for point, point_dist in full_distances_table[boundery_point].items():\n",
    "\n",
    "            if point in boundery_points or point in core_points:\n",
    "                    if point_dist < min_dist_point['dist']:\n",
    "                        min_dist_point = {'closest_point': point, 'dist': point_dist}\n",
    "        \n",
    "        relevant_component = points_comps[min_dist_point['closest_point']] \n",
    "        points_comps[boundery_point] =  relevant_component      \n",
    "        \n",
    "        if relevant_component != -1:\n",
    "            connected_components[relevant_component].append(boundery_point)\n",
    "    \n",
    "    components = [-1] * len(mX)\n",
    "    i = 0 \n",
    "    for comp, comp_lst in connected_components.items():\n",
    "        for point in comp_lst:\n",
    "            components[point] = i\n",
    "        i+= 1\n",
    "    return components, core_points    \n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Comparison\n",
    "Consider the following datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:01:14.121323Z",
     "start_time": "2022-03-20T15:01:13.758410Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N   = 1000\n",
    "A   = np.array([[0.6, -0.6],\n",
    "                [-0.4, 0.8]]);\n",
    "mX1 = datasets.make_circles(n_samples=N, noise=0.02)      [0]\n",
    "mX2 = datasets.make_moons  (n_samples=N, noise=0.05)      [0]\n",
    "mX3 = datasets.make_blobs  (n_samples=N, random_state=170)[0] @ A\n",
    "mX4 = datasets.make_blobs  (n_samples=N, random_state=170, cluster_std=[.8, 2, .4])[0] \n",
    "mX5 = np.load('clusterable_data.npy')\n",
    "\n",
    "lDatasets = [mX1, mX2, mX3, mX4, mX5]\n",
    "\n",
    "fig, _ = plt.subplots(1, 5, figsize=(12, 3))\n",
    "for ii, ax in enumerate(fig.axes):\n",
    "    mXi = lDatasets[ii]\n",
    "    ax.scatter(*mXi.T, c='lime', s=15, edgecolor='k')\n",
    "    ax.axis   ('equal')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show        ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the following methods and compare the obtained results:\n",
    "* **K-Means**\n",
    "* **GMM**\n",
    "* **DBSCAN**\n",
    "* Hierarchical clustering\n",
    "* HDBSCAN\n",
    "\n",
    "For the first three, use your own implementation.  \n",
    "For the last two, you may use existing libraries: [Hierarchical clustering (sk-learn)](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering), [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html).\n",
    "    \n",
    "**Note:** The datasets are transposed, e.g `mX1.shape = (1000, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-20T15:09:03.824893Z",
     "start_time": "2022-03-20T15:09:03.816886Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here...\n",
    "'''\n",
    "import hdbscan\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "hyper_parameters =  {0: {'r': .09, 'Z': 3, 'k': 2},\n",
    "                     1: {'r': .2, 'Z': 4, 'k': 2},\n",
    "                     2: {'r': .35, 'Z': 12, 'k': 3},\n",
    "                     3: {'r': 1.2, 'Z': 12, 'k': 3},\n",
    "                     4: {'r': 0.04, 'Z': 10, 'k': 3},\n",
    "    \n",
    "                    }\n",
    "\n",
    "\n",
    "for i, mX in enumerate(lDatasets):\n",
    "\n",
    "    KMeans_vC = KMeans(mX, hyper_parameters[i]['k'])\n",
    "    GMM_EM_vC, lW, lMu, lSig = GMM_EM(mX, hyper_parameters[i]['k'])\n",
    "    DBSCAN_vC, core_points = DBSCAN(mX, Z=hyper_parameters[i]['Z'], r=hyper_parameters[i]['r'])\n",
    "    HDBSCAN_vC = hdbscan.HDBSCAN(min_cluster_size=hyper_parameters[i]['Z'], cluster_selection_epsilon=hyper_parameters[i]['r']).fit(mX).labels_\n",
    "    hierarchical_vC = AgglomerativeClustering(n_clusters=hyper_parameters[i]['k']).fit(mX).labels_\n",
    "\n",
    "    Nc = hyper_parameters[i]['k'] + 1\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,5))\n",
    "    plt.figure()\n",
    "    \n",
    "    axes[0].scatter(mX[:,0], mX[:,1], s=100, c=KMeans_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[0].set_title('KMeans')\n",
    "    axes[1].scatter(mX[:,0], mX[:,1], s=100, c=GMM_EM_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[1].set_title('GMM')\n",
    "    axes[2].scatter(mX[:,0], mX[:,1], s=100, c=DBSCAN_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[2].set_title('DBSCAN')\n",
    "    axes[3].scatter(mX[:,0], mX[:,1], s=100, c=HDBSCAN_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[3].set_title('HDBSCAN')\n",
    "    axes[4].scatter(mX[:,0], mX[:,1], s=100, c=hierarchical_vC, edgecolor='k', cmap=matplotlib.cm.get_cmap('gist_rainbow', Nc))\n",
    "    axes[4].set_title('Hierarchical')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
